# -*- coding: utf-8 -*-
"""
ğŸ—ºï¸ ì„œìš¸ì‹œ ìŒì‹ì  ì¶”ì²œìš© ë„ë³´ ê¶Œì—­ ìƒì„± ë„êµ¬

ì´ ëª¨ë“ˆì€ ì„œìš¸ì‹œ ì „ì²´ë¥¼ H3 í•´ìƒë„ 10ìœ¼ë¡œ ë¹ˆí‹ˆì—†ì´ ì»¤ë²„í•œ í›„,
ë„ë³´ ê±°ë¦¬ ê¸°ë°˜ í´ëŸ¬ìŠ¤í„°ë§ì„ í†µí•´ ìŒì‹ì  ì¶”ì²œì— ì í•©í•œ ê¶Œì—­ì„ ìƒì„±í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- ğŸ™ï¸ OSMnxë¡œ ì„œìš¸ì‹œ í–‰ì •ê²½ê³„ ìë™ íšë“
- ğŸ“ H3 í•´ìƒë„ 10 ê¸°ë°˜ ì„œìš¸ì‹œ ì „ì²´ ì»¤ë²„ë¦¬ì§€
- ğŸš¶ OSRM ë„ë³´ ê±°ë¦¬ ê³„ì‚° (Haversine ë°±ì—…)
- ğŸ”— ê·¸ë˜í”„ ê¸°ë°˜ ì—°ê²° ìš”ì†Œ ë¶„ì„
- ğŸ“ CSV/GeoJSON ê²°ê³¼ ì¶œë ¥
- ğŸ“ˆ ì‹¤ì‹œê°„ ì§„í–‰ìƒí™© ì¶”ì 

ì‘ì„±ì: yamyam-lab
ë²„ì „: 3.0 (ìŒì‹ì  ì¶”ì²œìš© ë„ë³´ ê¶Œì—­ ìƒì„± íŠ¹í™”)
"""

import hashlib
import json
import logging
import math
import pickle
from collections import deque
from datetime import datetime
from pathlib import Path
from time import time
from typing import Dict, List, Optional, Set, Tuple

import h3  # pip install h3
import networkx as nx
import osmnx as ox  # pip install osmnx
import pandas as pd
import requests
from shapely.geometry import Polygon
from tqdm import tqdm

from yamyam_lab.data.base import BaseDatasetLoader
from yamyam_lab.data.config import DataConfig

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

# ---------------------------
# H3 v3/v4 í˜¸í™˜ì„± ì–´ëŒ‘í„°
# ---------------------------
_HAS_V4 = hasattr(h3, "latlng_to_cell")  # v4ì´ë©´ True


def h3_geo_to_cell(lat: float, lon: float, res: int) -> str:
    """H3 v3/v4 í˜¸í™˜ ì¢Œí‘œâ†’ì…€ ë³€í™˜"""
    return h3.latlng_to_cell(lat, lon, res) if _HAS_V4 else h3.geo_to_h3(lat, lon, res)


def h3_cell_to_boundary_latlon(cell_id: str) -> List[Tuple[float, float]]:
    """H3 v3/v4 í˜¸í™˜ ì…€â†’ê²½ê³„ì¢Œí‘œ ë³€í™˜ (lat, lon) ìˆœì„œ"""
    if _HAS_V4:
        return h3.cell_to_boundary(cell_id)
    else:
        return h3.h3_to_geo_boundary(cell_id, geo_json=False)


def h3_neighbors(cell_id: str, k: int = 1) -> Set[str]:
    """H3 v3/v4 í˜¸í™˜ k-ring ì´ì›ƒ ì…€"""
    if _HAS_V4:
        s = set(h3.grid_disk(cell_id, k))
    else:
        s = set(h3.k_ring(cell_id, k))
    s.discard(cell_id)
    return s


# ---------------------------
# OSRM Distance Cache Manager
# ---------------------------
class OSRMDistanceCache:
    """OSRM ê±°ë¦¬ ìºì‹œë¥¼ íŒŒì¼ë¡œ ì €ì¥/ë¡œë“œí•˜ëŠ” ê´€ë¦¬ í´ë˜ìŠ¤"""

    def __init__(self, cache_dir: str = "cache", region_name: str = "default"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self.region_name = region_name
        self.cache: Dict[Tuple[str, str], float] = {}
        self.cache_file = self.cache_dir / f"osrm_distance_cache_{region_name}.pkl"
        self.metadata_file = (
            self.cache_dir / f"osrm_distance_cache_{region_name}_metadata.json"
        )
        self.api_calls_count = 0
        self.cache_hits_count = 0

    def _generate_cache_key(
        self, lat1: float, lon1: float, lat2: float, lon2: float, profile: str = "foot"
    ) -> str:
        """ì¢Œí‘œì™€ í”„ë¡œí•„ì„ ê¸°ë°˜ìœ¼ë¡œ ìºì‹œ í‚¤ ìƒì„±"""
        # ì†Œìˆ˜ì  6ìë¦¬ë¡œ ë°˜ì˜¬ë¦¼í•˜ì—¬ í‚¤ ìƒì„± (ì•½ 10cm ì •ë°€ë„)
        coords = f"{lat1:.6f},{lon1:.6f},{lat2:.6f},{lon2:.6f},{profile}"
        return hashlib.md5(coords.encode()).hexdigest()

    def load_cache(self) -> bool:
        """ìºì‹œ íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ"""
        try:
            if self.cache_file.exists():
                with open(self.cache_file, "rb") as f:
                    self.cache = pickle.load(f)

                # ë©”íƒ€ë°ì´í„° ë¡œë“œ
                metadata = {}
                if self.metadata_file.exists():
                    with open(self.metadata_file, "r", encoding="utf-8") as f:
                        metadata = json.load(f)

                logger.info(f"OSRM ê±°ë¦¬ ìºì‹œ ë¡œë“œ ì™„ë£Œ: {len(self.cache):,}ê°œ í•­ëª©")
                if metadata:
                    logger.info(f"ìºì‹œ ìƒì„±ì¼: {metadata.get('created_at', 'Unknown')}")
                    logger.info(
                        f"ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: {metadata.get('last_updated', 'Unknown')}"
                    )

                return True
            else:
                logger.info("ê¸°ì¡´ OSRM ê±°ë¦¬ ìºì‹œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
                return False

        except Exception as e:
            logger.warning(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}. ìƒˆë¡œìš´ ìºì‹œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
            self.cache = {}
            return False

    def save_cache(self) -> bool:
        """ìºì‹œë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        try:
            # ìºì‹œ ë°ì´í„° ì €ì¥
            with open(self.cache_file, "wb") as f:
                pickle.dump(self.cache, f)

            # ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata = {
                "region_name": self.region_name,
                "cache_size": len(self.cache),
                "created_at": datetime.now().isoformat(),
                "last_updated": datetime.now().isoformat(),
                "api_calls_this_session": self.api_calls_count,
                "cache_hits_this_session": self.cache_hits_count,
                "cache_hit_rate": f"{self.cache_hits_count / max(1, self.cache_hits_count + self.api_calls_count) * 100:.1f}%",
            }

            with open(self.metadata_file, "w", encoding="utf-8") as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)

            logger.info(f"OSRM ê±°ë¦¬ ìºì‹œ ì €ì¥ ì™„ë£Œ: {len(self.cache):,}ê°œ í•­ëª©")
            logger.info(
                f"ì´ë²ˆ ì„¸ì…˜ API í˜¸ì¶œ: {self.api_calls_count}íšŒ, ìºì‹œ íˆíŠ¸: {self.cache_hits_count}íšŒ"
            )

            return True

        except Exception as e:
            logger.error(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    def get_distance(
        self,
        lat1: float,
        lon1: float,
        lat2: float,
        lon2: float,
        base_url: str = "https://router.project-osrm.org",
        profile: str = "foot",
        timeout: float = 5.0,
    ) -> Optional[float]:
        """ìºì‹œë¥¼ í™œìš©í•œ ê±°ë¦¬ ì¡°íšŒ"""
        cache_key = self._generate_cache_key(lat1, lon1, lat2, lon2, profile)

        # ìºì‹œì—ì„œ ì¡°íšŒ
        if cache_key in self.cache:
            self.cache_hits_count += 1
            return self.cache[cache_key]

        # ìºì‹œì— ì—†ìœ¼ë©´ API í˜¸ì¶œ
        self.api_calls_count += 1
        distance = osrm_distance_m(lat1, lon1, lat2, lon2, base_url, profile, timeout)

        # ì„±ê³µí•œ ê²½ìš° ìºì‹œì— ì €ì¥
        if distance is not None:
            self.cache[cache_key] = distance

        return distance

    def get_stats(self) -> Dict:
        """ìºì‹œ í†µê³„ ë°˜í™˜"""
        total_requests = self.api_calls_count + self.cache_hits_count
        hit_rate = (self.cache_hits_count / max(1, total_requests)) * 100

        return {
            "cache_size": len(self.cache),
            "api_calls": self.api_calls_count,
            "cache_hits": self.cache_hits_count,
            "hit_rate_percent": hit_rate,
            "cache_file": str(self.cache_file),
            "file_exists": self.cache_file.exists(),
        }


# ---------------------------
# Distance utilities
# ---------------------------
def haversine_m(lat1: float, lon1: float, lat2: float, lon2: float) -> float:
    """Haversine ì§ì„ ê±°ë¦¬ (meters)."""
    R = 6371000.0
    p1, p2 = math.radians(lat1), math.radians(lat2)
    dphi = math.radians(lat2 - lat1)
    dlmb = math.radians(lon2 - lon1)
    a = (
        math.sin(dphi / 2.0) ** 2
        + math.cos(p1) * math.cos(p2) * math.sin(dlmb / 2.0) ** 2
    )
    return 2 * R * math.asin(math.sqrt(a))


def osrm_route_distance(
    coords: List[Tuple[float, float]],
    base_url: str = "https://router.project-osrm.org",
    profile: str = "foot",
    overview: str = "false",
    timeout: float = 5.0,
) -> Optional[Dict]:
    """
    OSRM /route APIë¡œ ì—¬ëŸ¬ ì¢Œí‘œë¥¼ ì‡ëŠ” ê²½ë¡œë¥¼ ìš”ì²­.
    coords: [(lon, lat), (lon, lat), ...]  # OSRMì€ ê²½ë„,ìœ„ë„ ìˆœì„œ!
    ë°˜í™˜: {
        "distance": float(ì´ê±°ë¦¬ m),
        "duration": float(ì´ì‹œê°„ s),
        "legs": List[{"distance": m, "duration": s}, ...],
        "waypoints": List[...]
    }  ë˜ëŠ” ì‹¤íŒ¨ ì‹œ None
    """
    if len(coords) < 2:
        raise ValueError("coords must contain at least 2 points")

    try:
        coord_str = ";".join([f"{lon:.6f},{lat:.6f}" for lon, lat in coords])
        url = f"{base_url}/route/v1/{profile}/{coord_str}"
        params = {
            "overview": overview,  # "false"|"simplified"|"full"
            "alternatives": "false",
            "steps": "false",
            "annotations": "false",
        }
        r = requests.get(url, params=params, timeout=timeout)
        r.raise_for_status()
        data = r.json()
        if data.get("code") != "Ok" or not data.get("routes"):
            return None

        route = data["routes"][0]
        total_distance = float(route["distance"])  # m
        total_duration = float(route["duration"])  # s
        legs_summary = []
        for leg in route.get("legs", []):
            legs_summary.append(
                {
                    "distance": float(leg["distance"]),
                    "duration": float(leg["duration"]),
                }
            )

        return {
            "distance": total_distance,
            "duration": total_duration,
            "legs": legs_summary,
            "waypoints": data.get("waypoints", []),
        }
    except Exception:
        return None


def osrm_distance_m(
    lat1: float,
    lon1: float,
    lat2: float,
    lon2: float,
    base_url: str = "https://router.project-osrm.org",
    profile: str = "foot",
    timeout: float = 5.0,
) -> Optional[float]:
    """
    2ì  ì „ìš© OSRM ë˜í¼. (driving/foot/cycling í”„ë¡œí•„ ì§€ì›)

    Args:
        lat1, lon1: ì²« ë²ˆì§¸ ì ì˜ ìœ„ë„, ê²½ë„
        lat2, lon2: ë‘ ë²ˆì§¸ ì ì˜ ìœ„ë„, ê²½ë„
        base_url: OSRM ì„œë²„ URL
        profile: ê²½ë¡œ í”„ë¡œí•„ (foot, driving, cycling)
        timeout: ìš”ì²­ íƒ€ì„ì•„ì›ƒ (ì´ˆ)

    Returns:
        ê±°ë¦¬(ë¯¸í„°) ë˜ëŠ” ì‹¤íŒ¨ ì‹œ None
    """
    try:
        res = osrm_route_distance(
            coords=[(lon1, lat1), (lon2, lat2)],
            base_url=base_url,
            profile=profile,
            timeout=timeout,
            overview="false",
        )
        return None if res is None else res["distance"]
    except Exception as e:
        logger.warning(f"OSRM distance calculation failed: {e}")
        return None


# ---------------------------
# Seoul boundary & H3 coverage
# ---------------------------
def get_region_boundary(region_name: str = "ì„œìš¸íŠ¹ë³„ì‹œ") -> Polygon:
    """OSMnxë¡œ ì§€ì •ëœ ì§€ì—­ì˜ í–‰ì •ê²½ê³„ ê°€ì ¸ì˜¤ê¸°"""
    logger.info(f"{region_name} í–‰ì •ê²½ê³„ë¥¼ OSMnxë¡œ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")

    # ë‹¤ì–‘í•œ ê²€ìƒ‰ì–´ ì‹œë„
    search_terms = []
    if region_name == "ì„œìš¸íŠ¹ë³„ì‹œ":
        search_terms = ["Seoul, South Korea", "ì„œìš¸íŠ¹ë³„ì‹œ, ëŒ€í•œë¯¼êµ­"]
    else:
        # êµ¬/ë™ ë‹¨ìœ„ëŠ” í•œê¸€ëª… ìš°ì„ 
        search_terms = [
            f"{region_name}, ëŒ€í•œë¯¼êµ­",
            f"{region_name}, ì„œìš¸íŠ¹ë³„ì‹œ, ëŒ€í•œë¯¼êµ­",
        ]

    gdf = None
    for search_term in search_terms:
        try:
            logger.info(f"ê²€ìƒ‰ì–´ ì‹œë„: {search_term}")
            gdf = ox.geocode_to_gdf(search_term)
            break
        except Exception as e:
            logger.warning(f"'{search_term}' ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            continue

    if gdf is None:
        raise ValueError(
            f"'{region_name}' ì§€ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì •í™•í•œ ì§€ì—­ëª…ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
        )

    # WGS84ë¡œ ë³€í™˜ ë° ë‹¨ì¼ í´ë¦¬ê³¤ìœ¼ë¡œ í†µí•©
    gdf = gdf.to_crs(epsg=4326)
    region_geom = gdf.unary_union
    logger.info(f"{region_name} ê²½ê³„ íšë“ ì™„ë£Œ: {region_geom.geom_type}")

    # ê²½ê³„ ì •ë³´ ì¶œë ¥
    bounds = gdf.bounds
    logger.info(f"ê²½ê³„ ì¢Œí‘œ: {bounds.iloc[0].to_dict()}")

    return region_geom


def cell_polygon(cell_id: str) -> Polygon:
    """H3 ì…€ ID â†’ Shapely Polygon (lon, lat ìˆœì„œ)"""
    boundary_latlon = h3_cell_to_boundary_latlon(cell_id)
    boundary_lonlat = [(lon, lat) for (lat, lon) in boundary_latlon]
    return Polygon(boundary_lonlat)


def flood_fill_region_cells(
    region_boundary: Polygon, resolution: int, region_name: str = "ì§€ì—­"
) -> Set[str]:
    """
    ì§€ì—­ ê²½ê³„ ë‚´ë¥¼ H3 ì…€ë¡œ ì™„ì „íˆ ì»¤ë²„í•˜ëŠ” flood-fill ì•Œê³ ë¦¬ì¦˜
    - ì§€ì—­ ì¤‘ì‹¬ì—ì„œ ì‹œì‘í•´ ì´ì›ƒìœ¼ë¡œ í™•ì¥
    - ì…€ê³¼ ê²½ê³„ê°€ êµì°¨í•˜ëŠ” ëª¨ë“  ì…€ì„ í¬í•¨
    """
    logger.info(f"{region_name}ì„ H3 í•´ìƒë„ {resolution}ë¡œ ì™„ì „ ì»¤ë²„ ì¤‘...")
    start_time = time()

    # ì‹œì‘ì : ì§€ì—­ ì¤‘ì‹¬
    center = region_boundary.representative_point()
    start_cell = h3_geo_to_cell(center.y, center.x, resolution)

    result = set()
    visited = set([start_cell])
    queue = deque([start_cell])

    while queue:
        current_cell = queue.popleft()
        cell_poly = cell_polygon(current_cell)

        # ê²½ê³„ì™€ êµì°¨í•˜ì§€ ì•Šìœ¼ë©´ ì œì™¸
        if not cell_poly.intersects(region_boundary):
            continue

        # êµì°¨í•˜ëŠ” ì…€ì€ í¬í•¨
        result.add(current_cell)

        # ì´ì›ƒ ì…€ë“¤ì„ íì— ì¶”ê°€
        for neighbor in h3_neighbors(current_cell, 1):
            if neighbor not in visited:
                visited.add(neighbor)
                queue.append(neighbor)

    elapsed = time() - start_time
    logger.info(f"{region_name} H3 ì»¤ë²„ë¦¬ì§€ ì™„ë£Œ: {len(result)}ê°œ ì…€ ({elapsed:.2f}s)")
    return result


def load_restaurant_data_with_dataloader(
    data_config: DataConfig, region_name: str = "ì„œìš¸íŠ¹ë³„ì‹œ"
) -> pd.DataFrame:
    """ê¸°ì¡´ DataLoaderë¥¼ ì‚¬ìš©í•˜ì—¬ ìŒì‹ì  ë°ì´í„° ë¡œë“œ ë° ì§€ì—­ í•„í„°ë§"""
    logger.info("DataLoaderë¥¼ ì‚¬ìš©í•˜ì—¬ ìŒì‹ì  ë°ì´í„° ë¡œë“œ ì¤‘...")
    start_time = time()

    try:
        # BaseDatasetLoaderë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ë¡œë“œ
        loader = BaseDatasetLoader(data_config)
        review, diner, diner_with_raw_category = loader.load_dataset()

        initial_count = len(diner)

        # ì„œìš¸ ì§€ì—­ í•„í„°ë§ (region_nameì— ë”°ë¼ ì¡°ì •)
        if region_name == "ì„œìš¸íŠ¹ë³„ì‹œ":
            filtered_df = diner[
                diner["diner_road_address"].str.contains("ì„œìš¸", na=False)
            ].copy()
        else:
            # íŠ¹ì • êµ¬/ë™ í•„í„°ë§
            filtered_df = diner[
                diner["diner_road_address"].str.contains(region_name, na=False)
            ].copy()

        # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ ë° íƒ€ì… ë³€í™˜
        required_columns = [
            "diner_idx",
            "diner_name",
            "diner_lat",
            "diner_lon",
            "diner_review_cnt",
            "diner_review_avg",
            "diner_road_address",
        ]

        # bayesian_scoreê°€ ìˆìœ¼ë©´ í¬í•¨, ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ìƒì„±
        if "bayesian_score" in filtered_df.columns:
            required_columns.append("bayesian_score")
        else:
            filtered_df["bayesian_score"] = 0.0
            required_columns.append("bayesian_score")

        filtered_df = filtered_df[required_columns].copy()

        # ìˆ«ì ì»¬ëŸ¼ë“¤ì„ ì•ˆì „í•˜ê²Œ ë³€í™˜
        filtered_df["diner_lat"] = pd.to_numeric(
            filtered_df["diner_lat"], errors="coerce"
        )
        filtered_df["diner_lon"] = pd.to_numeric(
            filtered_df["diner_lon"], errors="coerce"
        )
        filtered_df["diner_review_cnt"] = (
            pd.to_numeric(filtered_df["diner_review_cnt"], errors="coerce")
            .fillna(0)
            .astype(int)
        )
        filtered_df["diner_review_avg"] = (
            pd.to_numeric(filtered_df["diner_review_avg"], errors="coerce")
            .fillna(0)
            .astype(float)
        )
        filtered_df["bayesian_score"] = (
            pd.to_numeric(filtered_df["bayesian_score"], errors="coerce")
            .fillna(0)
            .astype(float)
        )

        # NaNì´ ìˆëŠ” í–‰ ì œê±°
        filtered_df = filtered_df.dropna(subset=["diner_lat", "diner_lon"])

        # ì¢Œí‘œ ë²”ìœ„ ê²€ì¦
        valid_coords = filtered_df[
            (filtered_df["diner_lat"] >= 37.0)
            & (filtered_df["diner_lat"] <= 38.0)
            & (filtered_df["diner_lon"] >= 126.0)
            & (filtered_df["diner_lon"] <= 128.0)
        ]

        elapsed = time() - start_time
        logger.info(f"ìŒì‹ì  ë°ì´í„° ë¡œë“œ ì™„ë£Œ ({elapsed:.2f}s)")
        logger.info(
            f"ì „ì²´: {initial_count:,}ê°œ â†’ {region_name}: {len(valid_coords):,}ê°œ ìŒì‹ì "
        )

        return valid_coords

    except Exception as e:
        logger.error(f"ìŒì‹ì  ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return pd.DataFrame()


def map_restaurants_to_h3_cells(
    restaurants_df: pd.DataFrame, resolution: int
) -> pd.DataFrame:
    """ìŒì‹ì ì„ H3 ì…€ì— ë§¤í•‘í•˜ê³  ì…€ë³„ ìŒì‹ì  í†µê³„ ê³„ì‚°"""
    logger.info("ìŒì‹ì ì„ H3 ì…€ì— ë§¤í•‘ ì¤‘...")
    start_time = time()

    if restaurants_df.empty:
        logger.warning("ìŒì‹ì  ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return pd.DataFrame()

    # ìŒì‹ì ì„ H3 ì…€ì— ë§¤í•‘
    restaurants_df = restaurants_df.copy()
    restaurants_df["cell_id"] = restaurants_df.apply(
        lambda row: h3_geo_to_cell(row["diner_lat"], row["diner_lon"], resolution),
        axis=1,
    )

    # ì…€ë³„ ìŒì‹ì  í†µê³„ ê³„ì‚°
    cell_stats = (
        restaurants_df.groupby("cell_id")
        .agg(
            {
                "diner_idx": "count",  # ìŒì‹ì  ìˆ˜
                "diner_review_cnt": "sum",  # ì´ ë¦¬ë·° ìˆ˜
                "diner_review_avg": "mean",  # í‰ê·  í‰ì 
                "bayesian_score": "mean",  # í‰ê·  ë² ì´ì§€ì•ˆ ì ìˆ˜
            }
        )
        .round(2)
    )

    cell_stats.columns = [
        "restaurant_count",
        "total_reviews",
        "avg_rating",
        "avg_bayesian_score",
    ]

    # ë°ì´í„° íƒ€ì… ì•ˆì „í•˜ê²Œ ë³€í™˜
    cell_stats["restaurant_count"] = cell_stats["restaurant_count"].astype(int)
    cell_stats["total_reviews"] = cell_stats["total_reviews"].fillna(0).astype(int)
    cell_stats["avg_rating"] = cell_stats["avg_rating"].fillna(0).astype(float).round(2)
    cell_stats["avg_bayesian_score"] = (
        cell_stats["avg_bayesian_score"].fillna(0).astype(float).round(3)
    )

    # ì…€ ì¤‘ì‹¬ ì¢Œí‘œ ì¶”ê°€
    def get_cell_center(cell_id):
        if _HAS_V4:
            return h3.cell_to_latlng(cell_id)
        else:
            return h3.h3_to_geo(cell_id)

    cell_centers = pd.DataFrame(
        [
            {
                "cell_id": cell_id,
                "cell_lat": get_cell_center(cell_id)[0],
                "cell_lon": get_cell_center(cell_id)[1],
            }
            for cell_id in cell_stats.index
        ]
    ).set_index("cell_id")

    # í†µê³„ì™€ ì¢Œí‘œ ê²°í•©
    result_df = cell_stats.join(cell_centers).reset_index()

    elapsed = time() - start_time
    logger.info(f"H3 ë§¤í•‘ ì™„ë£Œ ({elapsed:.2f}s): {len(result_df)}ê°œ ì…€ì— ìŒì‹ì  ë¶„í¬")
    logger.info(f"ìŒì‹ì ì´ ìˆëŠ” ì…€: {len(result_df):,}ê°œ")
    logger.info(f"ì…€ë‹¹ í‰ê·  ìŒì‹ì  ìˆ˜: {result_df['restaurant_count'].mean():.1f}ê°œ")

    return result_df


def region_cells_to_dataframe(
    cell_ids: Set[str], restaurants_df: pd.DataFrame = None, resolution: int = 10
) -> pd.DataFrame:
    """ì§€ì—­ H3 ì…€ ì§‘í•©ì„ DataFrameìœ¼ë¡œ ë³€í™˜ (ìŒì‹ì  ì •ë³´ í¬í•¨)"""
    logger.info("H3 ì…€ ì •ë³´ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜ ì¤‘...")
    start_time = time()

    # ê¸°ë³¸ ì…€ ì •ë³´ ìƒì„±
    cells_data = []
    for cell_id in tqdm(cell_ids, desc="ì…€ ì •ë³´ ë³€í™˜"):
        if _HAS_V4:
            lat, lon = h3.cell_to_latlng(cell_id)
        else:
            lat, lon = h3.h3_to_geo(cell_id)
        cells_data.append(
            {
                "cell_id": cell_id,
                "cell_lat": lat,
                "cell_lon": lon,
                "point_count": 1,  # ê¸°ë³¸ ê°€ì¤‘ì¹˜
            }
        )

    result_df = pd.DataFrame(cells_data)

    # ìŒì‹ì  ë°ì´í„°ê°€ ìˆìœ¼ë©´ ë§¤í•‘
    if restaurants_df is not None and not restaurants_df.empty:
        logger.info("ìŒì‹ì  ì •ë³´ë¥¼ ì…€ì— ë§¤í•‘ ì¤‘...")
        restaurants_df.dropna(subset=["diner_lat", "diner_lon"], inplace=True)
        restaurant_cells = map_restaurants_to_h3_cells(restaurants_df, resolution)

        # ìŒì‹ì  ì •ë³´ ì¡°ì¸ (left joinìœ¼ë¡œ ëª¨ë“  ì…€ ìœ ì§€)
        result_df = result_df.merge(
            restaurant_cells[
                [
                    "cell_id",
                    "restaurant_count",
                    "total_reviews",
                    "avg_rating",
                    "avg_bayesian_score",
                ]
            ],
            on="cell_id",
            how="left",
        )

        # ìŒì‹ì ì´ ì—†ëŠ” ì…€ì€ 0ìœ¼ë¡œ ì±„ìš°ê¸°
        result_df["restaurant_count"] = (
            result_df["restaurant_count"].fillna(0).astype(int)
        )
        result_df["total_reviews"] = result_df["total_reviews"].fillna(0).astype(int)
        result_df["avg_rating"] = (
            result_df["avg_rating"].fillna(0).astype(float).round(2)
        )
        result_df["avg_bayesian_score"] = (
            result_df["avg_bayesian_score"].fillna(0).astype(float).round(3)
        )

        # ìŒì‹ì  ìˆ˜ë¥¼ point_countì— ë°˜ì˜ (ê°€ì¤‘ì¹˜)
        result_df["point_count"] = (
            result_df["restaurant_count"].astype(int) + 1
        )  # ìµœì†Œ 1ê°œëŠ” ë³´ì¥

        logger.info(
            f"ìŒì‹ì ì´ ìˆëŠ” ì…€: {(result_df['restaurant_count'] > 0).sum():,}ê°œ"
        )
        logger.info(
            f"ìŒì‹ì ì´ ì—†ëŠ” ì…€: {(result_df['restaurant_count'] == 0).sum():,}ê°œ"
        )

    elapsed = time() - start_time
    logger.info(f"DataFrame ë³€í™˜ ì™„ë£Œ ({elapsed:.2f}s)")
    return result_df


# ---------------------------
# H3 helpers (legacy support)
# ---------------------------
def to_h3_cells(
    df: pd.DataFrame, lat_col: str, lon_col: str, resolution: int
) -> pd.DataFrame:
    """í¬ì¸íŠ¸ â†’ H3 ì…€ ì§‘ê³„ ë° ì…€ ì¤‘ì‹¬ ì¢Œí‘œ (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)"""
    logger.info(f"Converting {len(df)} points to H3 cells (resolution={resolution})")
    start_time = time()

    tmp = df[[lat_col, lon_col]].dropna().copy()
    logger.info(f"Processing {len(tmp)} valid coordinates after removing NaN")

    tmp["cell_id"] = tmp.apply(
        lambda r: h3_geo_to_cell(r[lat_col], r[lon_col], resolution), axis=1
    )
    agg = tmp.groupby("cell_id").size().reset_index(name="point_count")

    # H3 v3/v4 í˜¸í™˜ ì…€ ì¤‘ì‹¬ ì¢Œí‘œ
    def get_cell_center(cell_id):
        if _HAS_V4:
            return pd.Series(h3.cell_to_latlng(cell_id))
        else:
            return pd.Series(h3.h3_to_geo(cell_id))

    agg[["cell_lat", "cell_lon"]] = agg["cell_id"].apply(get_cell_center)

    elapsed = time() - start_time
    logger.info(f"Generated {len(agg)} H3 cells in {elapsed:.2f}s")
    return agg[["cell_id", "cell_lat", "cell_lon", "point_count"]]


def neighbor_candidates(cell_id: str, k: int) -> Set[str]:
    """H3 k-ring ì´ì›ƒ í›„ë³´ (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)"""
    return h3_neighbors(cell_id, k)


# ---------------------------
# Graph construction
# ---------------------------
def build_cell_graph(
    cells_df: pd.DataFrame,
    distance_threshold_m: float,
    distance_metric: str = "osrm_then_haversine",
    osrm_base_url: str = "https://router.project-osrm.org",
    osrm_profile: str = "foot",
    osrm_timeout: float = 5.0,
    kring: int = 1,
    osrm_cache: Optional[OSRMDistanceCache] = None,
) -> Tuple[nx.Graph, Dict[Tuple[str, str], float]]:
    """
    ì…€ ì¤‘ì‹¬ì  ê¸°ë°˜ ê·¸ë˜í”„ êµ¬ì„±.
    - kring > 0: H3 k-ring ì´ì›ƒ ì…€ë§Œ ê±°ë¦¬ ê³„ì‚° â†’ ê³ ì†
    - kring == 0: ëª¨ë“  ìŒ ë¹„êµ (ë°ì´í„° ì ì„ ë•Œë§Œ ê¶Œì¥)
    distance_metric: "osrm" | "haversine" | "osrm_then_haversine"
    """
    logger.info(
        f"Building graph for {len(cells_df)} cells (distance_metric={distance_metric}, threshold={distance_threshold_m}m)"
    )
    start_time = time()

    G = nx.Graph()
    # ë…¸ë“œ ì¶”ê°€
    logger.info("Adding nodes to graph...")
    for _, row in tqdm(cells_df.iterrows(), total=len(cells_df), desc="Adding nodes"):
        G.add_node(
            row["cell_id"],
            lat=row["cell_lat"],
            lon=row["cell_lon"],
            point_count=int(row["point_count"]),
        )

    # ë¹ ë¥¸ ì¡°íšŒìš© dict
    latlon = {
        r["cell_id"]: (float(r["cell_lat"]), float(r["cell_lon"]))
        for _, r in cells_df.iterrows()
    }
    present = set(latlon.keys())

    # ê±°ë¦¬ ìºì‹œ (ë¡œì»¬ ì„¸ì…˜ìš©, OSRM ìºì‹œì™€ ë³„ë„)
    dcache: Dict[Tuple[str, str], float] = {}

    def cell_distance(a: str, b: str) -> Optional[float]:
        key = (a, b) if a < b else (b, a)
        if key in dcache:
            return dcache[key]

        lat_a, lon_a = latlon[a]
        lat_b, lon_b = latlon[b]

        if distance_metric == "osrm":
            if osrm_cache:
                d = osrm_cache.get_distance(
                    lat_a,
                    lon_a,
                    lat_b,
                    lon_b,
                    base_url=osrm_base_url,
                    profile=osrm_profile,
                    timeout=osrm_timeout,
                )
            else:
                d = osrm_distance_m(
                    lat_a,
                    lon_a,
                    lat_b,
                    lon_b,
                    base_url=osrm_base_url,
                    profile=osrm_profile,
                    timeout=osrm_timeout,
                )
        elif distance_metric == "haversine":
            d = haversine_m(lat_a, lon_a, lat_b, lon_b)
        else:  # osrm_then_haversine
            if osrm_cache:
                d = osrm_cache.get_distance(
                    lat_a,
                    lon_a,
                    lat_b,
                    lon_b,
                    base_url=osrm_base_url,
                    profile=osrm_profile,
                    timeout=osrm_timeout,
                )
            else:
                d = osrm_distance_m(
                    lat_a,
                    lon_a,
                    lat_b,
                    lon_b,
                    base_url=osrm_base_url,
                    profile=osrm_profile,
                    timeout=osrm_timeout,
                )
            if d is None:
                d = haversine_m(lat_a, lon_a, lat_b, lon_b)

        if d is not None:
            dcache[key] = d
        return d

    # ì—£ì§€ ì¶”ê°€
    logger.info("Computing distances and adding edges...")
    edges_added = 0
    distance_calculations = 0

    if kring > 0:
        # ê° ì…€ì˜ k-ring ì´ì›ƒë§Œ ë¹„êµ
        logger.info(f"Using k-ring strategy with k={kring}")
        for cid in tqdm(present, desc="Processing cells"):
            for nb in neighbor_candidates(cid, kring):
                if nb in present and cid < nb:  # ì¤‘ë³µ ê³„ì‚° ë°©ì§€
                    distance_calculations += 1
                    d = cell_distance(cid, nb)
                    if d is not None and d <= distance_threshold_m:
                        G.add_edge(cid, nb, distance_m=d)
                        edges_added += 1
    else:
        # ëª¨ë“  ìŒ ë¹„êµ (N^2) â€” ë°ì´í„°ê°€ ë§¤ìš° ì ì„ ë•Œë§Œ
        logger.info("Using all-pairs strategy (O(nÂ²))")
        ids = sorted(present)
        total_pairs = len(ids) * (len(ids) - 1) // 2
        with tqdm(total=total_pairs, desc="Computing distances") as pbar:
            for i, a in enumerate(ids):
                for b in ids[i + 1 :]:
                    distance_calculations += 1
                    d = cell_distance(a, b)
                    if d is not None and d <= distance_threshold_m:
                        G.add_edge(a, b, distance_m=d)
                        edges_added += 1
                    pbar.update(1)

    elapsed = time() - start_time
    logger.info(f"Graph construction completed in {elapsed:.2f}s")
    logger.info(
        f"Nodes: {G.number_of_nodes()}, Edges: {edges_added}, Distance calculations: {distance_calculations}"
    )
    logger.info(
        f"Local cache hit rate: {(distance_calculations - len(dcache)) / distance_calculations * 100:.1f}%"
    )

    # OSRM ìºì‹œ í†µê³„ ì¶œë ¥
    if osrm_cache:
        cache_stats = osrm_cache.get_stats()
        logger.info(
            f"OSRM cache stats: {cache_stats['cache_hits']} hits, {cache_stats['api_calls']} API calls"
        )
        logger.info(f"OSRM cache hit rate: {cache_stats['hit_rate_percent']:.1f}%")

    return G, dcache


def calculate_region_diameter(G: nx.Graph, nodes: List[str]) -> float:
    """ê¶Œì—­ ë‚´ ìµœëŒ€ ê±°ë¦¬(ì§€ë¦„) ê³„ì‚°"""
    if len(nodes) < 2:
        return 0.0

    max_distance = 0.0
    # ì„œë¸Œê·¸ë˜í”„ ìƒì„±
    subgraph = G.subgraph(nodes)

    # ëª¨ë“  ìŒ ê°„ì˜ ìµœë‹¨ ê²½ë¡œ ì¤‘ ìµœëŒ€ê°’ ì°¾ê¸°
    try:
        # ì—°ê²°ëœ ê·¸ë˜í”„ì—ì„œë§Œ ê³„ì‚°
        if nx.is_connected(subgraph):
            # ëª¨ë“  ìŒ ê°„ ìµœë‹¨ ê²½ë¡œ ê³„ì‚° (edge weight ì‚¬ìš©)
            path_lengths = dict(
                nx.all_pairs_dijkstra_path_length(subgraph, weight="distance_m")
            )
            for source in path_lengths:
                for target in path_lengths[source]:
                    if source != target:
                        max_distance = max(max_distance, path_lengths[source][target])
    except:
        # ê³„ì‚° ì‹¤íŒ¨ ì‹œ 0 ë°˜í™˜
        pass

    return max_distance


def split_large_region(
    G: nx.Graph,
    nodes: List[str],
    max_distance_m: float,
    max_cells_per_region: Optional[int] = None,
) -> List[List[str]]:
    """
    í° ê¶Œì—­ì„ ì‘ì€ ê¶Œì—­ë“¤ë¡œ ë¶„í• .
    - ì œì•½ 1: ê¶Œì—­ ì§€ë¦„ <= max_distance_m
    - ì œì•½ 2: ê¶Œì—­ ì…€ ìˆ˜ <= max_cells_per_region (ì˜µì…˜)
    ë‘ ì œì•½ì„ ëª¨ë‘ ë§Œì¡±í•  ë•Œê¹Œì§€ Girvan-Newmanìœ¼ë¡œ ì¬ê·€ ë¶„í• .
    """
    if len(nodes) < 2:
        return [nodes]

    # ì„œë¸Œê·¸ë˜í”„ ìƒì„±
    subgraph = G.subgraph(nodes)

    # í˜„ì¬ ê¶Œì—­ì˜ ì§€ë¦„ ê³„ì‚°
    diameter = calculate_region_diameter(G, nodes)

    meets_size = (
        True if max_cells_per_region is None else (len(nodes) <= max_cells_per_region)
    )
    meets_diameter = diameter <= max_distance_m

    if meets_size and meets_diameter:
        return [nodes]  # ë¶„í•  ë¶ˆí•„ìš”

    # Girvan-Newman ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì»¤ë®¤ë‹ˆí‹° ë¶„í• 
    try:
        communities = list(nx.community.girvan_newman(subgraph))
        if len(communities) > 0:
            # ì²« ë²ˆì§¸ ë¶„í•  ê²°ê³¼ ì‚¬ìš©
            split_communities = communities[0]
            result = []

            for community in split_communities:
                community_nodes = list(community)
                # ì¬ê·€ì ìœ¼ë¡œ ë” ë¶„í• ì´ í•„ìš”í•œì§€ í™•ì¸
                sub_regions = split_large_region(
                    G,
                    community_nodes,
                    max_distance_m,
                    max_cells_per_region,
                )
                result.extend(sub_regions)

            return result
        else:
            return [nodes]
    except:
        # ë¶„í•  ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
        return [nodes]


def label_regions_from_components(
    G: nx.Graph,
    min_cells_per_region: int = 1,
    max_region_distance_m: float = 2000.0,
    max_cells_per_region: Optional[int] = None,
) -> Dict[str, int]:
    """
    ì—°ê²°ìš”ì†Œë¥¼ region_idë¡œ ë¼ë²¨ë§.
    - ì‘ì€ ì¡°ê°(<min_cells)ì€ -1(ë…¸ì´ì¦ˆ)
    - í° ê¶Œì—­ì€ (ì§€ë¦„, ìµœëŒ€ ì…€ ìˆ˜) ì œì•½ì„ ë§Œì¡±í•˜ë„ë¡ ë¶„í• 
    """
    logger.info(
        f"Analyzing connected components (min_cells={min_cells_per_region}, max_distance={max_region_distance_m}m, max_cells={max_cells_per_region})"
    )
    start_time = time()

    regions: Dict[str, int] = {}
    comp_id = 0
    components = list(nx.connected_components(G))

    large_components = 0
    small_components = 0
    split_regions = 0

    for comp in tqdm(components, desc="Processing components"):
        comp_nodes = list(comp)

        if len(comp_nodes) < min_cells_per_region:
            # ì‘ì€ ì¡°ê°ì€ ë…¸ì´ì¦ˆë¡œ ì²˜ë¦¬
            for cid in comp_nodes:
                regions[cid] = -1
            small_components += 1
            continue

        # í° ê¶Œì—­ ë¶„í•  ì‹œë„ (ì§€ë¦„/ì…€ ìˆ˜ ì œì•½ ë™ì‹œ ë§Œì¡±)
        sub_regions = split_large_region(
            G,
            comp_nodes,
            max_region_distance_m,
            max_cells_per_region=max_cells_per_region,
        )

        for sub_region in sub_regions:
            if len(sub_region) >= min_cells_per_region:
                for cid in sub_region:
                    regions[cid] = comp_id
                comp_id += 1
                large_components += 1

                if len(sub_regions) > 1:
                    split_regions += 1
            else:
                # ë¶„í•  í›„ì—ë„ ì‘ì€ ì¡°ê°ì€ ë…¸ì´ì¦ˆ
                for cid in sub_region:
                    regions[cid] = -1
                small_components += 1

    elapsed = time() - start_time
    logger.info(f"Region labeling completed in {elapsed:.2f}s")
    logger.info(
        f"Found {large_components} valid regions, {small_components} small components (noise)"
    )
    if split_regions > 0:
        logger.info(f"Split {split_regions} large regions due to distance constraint")

    return regions


def reassign_orphan_cells(
    G: nx.Graph,
    cells_df: pd.DataFrame,
    region_labels: Dict[str, int],
    max_reassign_distance_m: float = 1500.0,
    osrm_cache: Optional[OSRMDistanceCache] = None,
) -> Dict[str, int]:
    """
    ê³ ì•„ ì…€(region_id = -1)ì„ ê°€ì¥ ê°€ê¹Œìš´ ì´ì›ƒ ê¶Œì—­ì— ì¬í• ë‹¹.

    Args:
        G: ì…€ ê·¸ë˜í”„
        cells_df: ì…€ ì •ë³´ DataFrame
        region_labels: ê¸°ì¡´ ê¶Œì—­ ë¼ë²¨ ë”•ì…”ë„ˆë¦¬
        max_reassign_distance_m: ì¬í• ë‹¹ ìµœëŒ€ ê±°ë¦¬ (ë¯¸í„°)

    Returns:
        ì—…ë°ì´íŠ¸ëœ ê¶Œì—­ ë¼ë²¨ ë”•ì…”ë„ˆë¦¬
    """
    logger.info("ê³ ì•„ ì…€(ë…¸ì´ì¦ˆ)ì„ ì´ì›ƒ ê¶Œì—­ì— ì¬í• ë‹¹ ì¤‘...")
    start_time = time()

    # ê³ ì•„ ì…€ê³¼ ìœ íš¨ ê¶Œì—­ ì…€ ë¶„ë¦¬
    orphan_cells = [cid for cid, rid in region_labels.items() if rid == -1]
    valid_cells = [cid for cid, rid in region_labels.items() if rid >= 0]

    if not orphan_cells:
        logger.info("ì¬í• ë‹¹í•  ê³ ì•„ ì…€ì´ ì—†ìŠµë‹ˆë‹¤.")
        return region_labels

    logger.info(f"ì¬í• ë‹¹ ëŒ€ìƒ: {len(orphan_cells)}ê°œ ê³ ì•„ ì…€")

    # ì…€ ì¢Œí‘œ ë§¤í•‘
    cell_coords = {
        row["cell_id"]: (row["cell_lat"], row["cell_lon"])
        for _, row in cells_df.iterrows()
    }

    updated_labels = region_labels.copy()
    reassigned_count = 0

    for orphan_cell in tqdm(orphan_cells, desc="ê³ ì•„ ì…€ ì¬í• ë‹¹"):
        if orphan_cell not in cell_coords:
            continue

        orphan_lat, orphan_lon = cell_coords[orphan_cell]
        best_region_id = -1
        best_distance = float("inf")
        best_neighbor_cell = None

        # k-ring ì´ì›ƒë“¤ ì¤‘ì—ì„œ ìœ íš¨ ê¶Œì—­ì— ì†í•œ ì…€ ì°¾ê¸°
        for k in range(1, 4):  # 1, 2, 3 ringê¹Œì§€ í™•ì¥ íƒìƒ‰
            neighbors = h3_neighbors(orphan_cell, k)

            for neighbor_cell in neighbors:
                if neighbor_cell in valid_cells:
                    neighbor_region = region_labels[neighbor_cell]
                    if neighbor_region >= 0:  # ìœ íš¨ ê¶Œì—­
                        neighbor_lat, neighbor_lon = cell_coords.get(
                            neighbor_cell, (None, None)
                        )
                        if neighbor_lat is None:
                            continue

                        # ë„ë³´ ê±°ë¦¬ ê³„ì‚°
                        distance = haversine_m(
                            orphan_lat, orphan_lon, neighbor_lat, neighbor_lon
                        )

                        # OSRM ë„ë³´ ê±°ë¦¬ë¡œ ë” ì •í™•í•˜ê²Œ ê³„ì‚° (ì„ íƒì )
                        if osrm_cache:
                            osrm_dist = osrm_cache.get_distance(
                                orphan_lat, orphan_lon, neighbor_lat, neighbor_lon
                            )
                        else:
                            osrm_dist = osrm_distance_m(
                                orphan_lat, orphan_lon, neighbor_lat, neighbor_lon
                            )
                        if osrm_dist is not None:
                            distance = osrm_dist

                        if (
                            distance < best_distance
                            and distance <= max_reassign_distance_m
                        ):
                            best_distance = distance
                            best_region_id = neighbor_region
                            best_neighbor_cell = neighbor_cell

            # ê°€ê¹Œìš´ ì´ì›ƒì„ ì°¾ì•˜ìœ¼ë©´ ë” ë©€ë¦¬ íƒìƒ‰í•˜ì§€ ì•ŠìŒ
            if best_region_id >= 0:
                break

        # ì¬í• ë‹¹ ìˆ˜í–‰
        if best_region_id >= 0:
            updated_labels[orphan_cell] = best_region_id
            reassigned_count += 1
            logger.debug(
                f"ì…€ {orphan_cell} â†’ ê¶Œì—­ {best_region_id} (ê±°ë¦¬: {best_distance:.0f}m, ì´ì›ƒ: {best_neighbor_cell})"
            )

    elapsed = time() - start_time
    logger.info(f"ê³ ì•„ ì…€ ì¬í• ë‹¹ ì™„ë£Œ ({elapsed:.2f}s)")
    logger.info(f"ì¬í• ë‹¹ ì„±ê³µ: {reassigned_count}/{len(orphan_cells)}ê°œ ì…€")
    logger.info(f"ë‚¨ì€ ê³ ì•„ ì…€: {len(orphan_cells) - reassigned_count}ê°œ")

    return updated_labels


def save_graph_analysis(
    G: nx.Graph,
    cells_df: pd.DataFrame,
    region_labels: Dict[str, int],
    distance_cache: Dict[Tuple[str, str], float],
    out_dir: str,
    filename_prefix: str,
) -> Dict[str, str]:
    """
    ê·¸ë˜í”„ì™€ ë¶„ì„ ê²°ê³¼ë¥¼ ì¤‘ê°„ ì €ì¥.

    Returns:
        ì €ì¥ëœ íŒŒì¼ ê²½ë¡œë“¤
    """
    logger.info("ê·¸ë˜í”„ì™€ ë¶„ì„ ê²°ê³¼ ì¤‘ê°„ ì €ì¥ ì¤‘...")
    start_time = time()

    output_path = Path(out_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    saved_files = {}

    try:
        # 1. NetworkX ê·¸ë˜í”„ ì €ì¥ (pickle)
        graph_path = output_path / f"{filename_prefix}_graph.pkl"
        with open(graph_path, "wb") as f:
            pickle.dump(G, f)
        saved_files["graph"] = str(graph_path)
        logger.info(f"ê·¸ë˜í”„ ì €ì¥: {graph_path.name}")

        # 2. ê±°ë¦¬ ìºì‹œ ì €ì¥ (pickle)
        cache_path = output_path / f"{filename_prefix}_distance_cache.pkl"
        with open(cache_path, "wb") as f:
            pickle.dump(distance_cache, f)
        saved_files["distance_cache"] = str(cache_path)
        logger.info(f"ê±°ë¦¬ ìºì‹œ ì €ì¥: {cache_path.name}")

        # 3. ì—£ì§€ ë¦¬ìŠ¤íŠ¸ CSV ì €ì¥ (ë¶„ì„ìš©)
        edges_data = []
        for u, v, data in G.edges(data=True):
            u_coords = (G.nodes[u]["lat"], G.nodes[u]["lon"])
            v_coords = (G.nodes[v]["lat"], G.nodes[v]["lon"])
            edges_data.append(
                {
                    "cell_a": u,
                    "cell_b": v,
                    "cell_a_lat": u_coords[0],
                    "cell_a_lon": u_coords[1],
                    "cell_b_lat": v_coords[0],
                    "cell_b_lon": v_coords[1],
                    "distance_m": data.get("distance_m", 0),
                    "region_a": region_labels.get(u, -1),
                    "region_b": region_labels.get(v, -1),
                    "is_intra_region": region_labels.get(u, -1)
                    == region_labels.get(v, -1)
                    and region_labels.get(u, -1) >= 0,
                }
            )

        edges_df = pd.DataFrame(edges_data)
        edges_path = output_path / f"{filename_prefix}_edges.csv"
        edges_df.to_csv(edges_path, index=False, encoding="utf-8")
        saved_files["edges"] = str(edges_path)
        logger.info(f"ì—£ì§€ ë¦¬ìŠ¤íŠ¸ ì €ì¥: {edges_path.name} ({len(edges_df)}ê°œ ì—£ì§€)")

        # 4. ë…¸ë“œ í†µê³„ CSV ì €ì¥
        nodes_data = []
        for node_id, data in G.nodes(data=True):
            neighbors = list(G.neighbors(node_id))
            region_id = region_labels.get(node_id, -1)
            nodes_data.append(
                {
                    "cell_id": node_id,
                    "cell_lat": data["lat"],
                    "cell_lon": data["lon"],
                    "point_count": data.get("point_count", 1),
                    "region_id": region_id,
                    "degree": G.degree(node_id),
                    "neighbors_count": len(neighbors),
                    "is_isolated": len(neighbors) == 0,
                    "is_orphan": region_id == -1,
                }
            )

        nodes_df = pd.DataFrame(nodes_data)
        nodes_path = output_path / f"{filename_prefix}_nodes.csv"
        nodes_df.to_csv(nodes_path, index=False, encoding="utf-8")
        saved_files["nodes"] = str(nodes_path)
        logger.info(f"ë…¸ë“œ í†µê³„ ì €ì¥: {nodes_path.name} ({len(nodes_df)}ê°œ ë…¸ë“œ)")

        # 5. ê¶Œì—­ í†µê³„ JSON ì €ì¥
        region_stats = {}
        for region_id in set(region_labels.values()):
            region_cells = [
                cid for cid, rid in region_labels.items() if rid == region_id
            ]
            if region_cells:
                region_nodes = [cid for cid in region_cells if cid in G.nodes]
                subgraph = G.subgraph(region_nodes)

                region_stats[str(region_id)] = {
                    "cell_count": len(region_cells),
                    "connected_cells": len(region_nodes),
                    "edges_count": subgraph.number_of_edges(),
                    "is_connected": nx.is_connected(subgraph)
                    if len(region_nodes) > 1
                    else True,
                    "diameter_m": calculate_region_diameter(G, region_nodes)
                    if len(region_nodes) > 1
                    else 0,
                    "is_orphan": region_id == -1,
                }

        stats_path = output_path / f"{filename_prefix}_region_stats.json"
        with open(stats_path, "w", encoding="utf-8") as f:
            json.dump(region_stats, f, ensure_ascii=False, indent=2)
        saved_files["region_stats"] = str(stats_path)
        logger.info(f"ê¶Œì—­ í†µê³„ ì €ì¥: {stats_path.name}")

        elapsed = time() - start_time
        logger.info(f"ì¤‘ê°„ ì €ì¥ ì™„ë£Œ ({elapsed:.2f}s): {len(saved_files)}ê°œ íŒŒì¼")

        return saved_files

    except Exception as e:
        logger.error(f"ì¤‘ê°„ ì €ì¥ ì‹¤íŒ¨: {e}")
        return {}


def attach_anchor_labels(
    cells_df: pd.DataFrame,
    region_labels: Dict[str, int],
    anchors_df: Optional[pd.DataFrame] = None,
    anchor_lat_col: str = "anchor_lat",
    anchor_lon_col: str = "anchor_lon",
) -> pd.DataFrame:
    """(ì„ íƒ) ê°€ì¥ ê°€ê¹Œìš´ ì•µì»¤ idë¥¼ ê° ì…€ì— ë¶€ì—¬."""
    logger.info("ì•µì»¤ ë¼ë²¨ ë¶€ì—¬ ì¤‘...")
    start_time = time()

    cells_df = cells_df.copy()
    cells_df["region_id"] = cells_df["cell_id"].map(region_labels)

    if anchors_df is None or anchors_df.empty:
        cells_df["anchor_id"] = None
        logger.info("ì•µì»¤ ë°ì´í„°ê°€ ì—†ì–´ ê±´ë„ˆëœ€")
        return cells_df

    logger.info(f"{len(anchors_df)}ê°œ ì•µì»¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœê·¼ì ‘ ì•µì»¤ ê³„ì‚° ì¤‘...")
    coords = list(anchors_df[[anchor_lat_col, anchor_lon_col]].to_records(index=False))

    def nearest_anchor(lat: float, lon: float) -> int:
        best_idx, best_d = -1, float("inf")
        for idx, (alat, alon) in enumerate(coords):
            d = haversine_m(lat, lon, float(alat), float(alon))
            if d < best_d:
                best_d, best_idx = d, idx
        return best_idx

    tqdm.pandas(desc="ì•µì»¤ ë¼ë²¨ ë¶€ì—¬")
    cells_df["anchor_id"] = cells_df.progress_apply(
        lambda r: nearest_anchor(r["cell_lat"], r["cell_lon"]), axis=1
    )

    elapsed = time() - start_time
    logger.info(f"ì•µì»¤ ë¼ë²¨ ë¶€ì—¬ ì™„ë£Œ ({elapsed:.2f}s)")
    return cells_df


def cells_to_geojson(cells_with_regions: pd.DataFrame, out_path: str) -> str:
    """ê° H3 ì…€ì„ Polygon Featureë¡œ ë‚´ë³´ë‚´ê¸° (region_id ë° ìŒì‹ì  ì •ë³´ í¬í•¨)."""
    features = []
    for _, r in cells_with_regions.iterrows():
        # H3 v3/v4 í˜¸í™˜ ê²½ê³„ ì¢Œí‘œ
        if _HAS_V4:
            boundary_coords = h3.cell_to_boundary(r["cell_id"])
            # GeoJSON í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (lon, lat ìˆœì„œ)
            boundary = [[lon, lat] for lat, lon in boundary_coords]
        else:
            boundary = h3.h3_to_geo_boundary(r["cell_id"], geo_json=True)

        # ê¸°ë³¸ ì†ì„±
        properties = {
            "cell_id": r["cell_id"],
            "region_id": int(r["region_id"]),
            "point_count": int(r["point_count"]),
            "anchor_id": None
            if pd.isna(r.get("anchor_id", None))
            else int(r["anchor_id"]),
        }

        # ìŒì‹ì  ì •ë³´ ì¶”ê°€ (ìˆëŠ” ê²½ìš°)
        if "restaurant_count" in r and not pd.isna(r["restaurant_count"]):
            properties.update(
                {
                    "restaurant_count": int(r["restaurant_count"]),
                    "total_reviews": int(r.get("total_reviews", 0)),
                    "avg_rating": float(r.get("avg_rating", 0)),
                    "avg_bayesian_score": float(r.get("avg_bayesian_score", 0)),
                }
            )

        poly = {
            "type": "Feature",
            "geometry": {"type": "Polygon", "coordinates": [boundary]},
            "properties": properties,
        }
        features.append(poly)

    fc = {"type": "FeatureCollection", "features": features}
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(fc, f, ensure_ascii=False, indent=2)
    return out_path


# ---------------------------
# Seoul Walking Region Builder
# ---------------------------
def build_walking_regions(
    data_config: DataConfig,
    region_name: str = "ì„œìš¸íŠ¹ë³„ì‹œ",
    resolution: int = 10,
    walking_threshold_m: float = 1000.0,
    max_region_distance_m: float = 2500.0,
    distance_metric: str = "osrm_then_haversine",
    osrm_base_url: str = "https://router.project-osrm.org",
    osrm_profile: str = "foot",
    osrm_timeout: float = 5.0,
    min_cells_per_region: int = 2,
    max_cells_per_region: Optional[int] = None,
    enable_orphan_reassign: bool = True,
    max_reassign_distance_m: Optional[float] = None,
    use_osrm_cache: bool = True,
    osrm_cache_dir: str = "osrm_cache",
    use_restaurant_data: bool = True,
    out_dir: Optional[str] = None,
    filename: str = None,
    kring: int = 1,
) -> pd.DataFrame:
    """ìŒì‹ì  ì¶”ì²œìš© ë„ë³´ ê¶Œì—­ ìƒì„±"""
    logger.info("=" * 60)
    logger.info(f"{region_name} ìŒì‹ì  ì¶”ì²œìš© ë„ë³´ ê¶Œì—­ ìƒì„± ì‹œì‘")
    logger.info("=" * 60)
    logger.info(f"ëŒ€ìƒ ì§€ì—­: {region_name}")
    logger.info(f"H3 í•´ìƒë„: {resolution}")
    logger.info(f"ë„ë³´ ê±°ë¦¬ ì„ê³„ê°’: {walking_threshold_m}m")
    logger.info(f"ìµœëŒ€ ê¶Œì—­ ê±°ë¦¬: {max_region_distance_m}m")
    logger.info(f"ê±°ë¦¬ ê³„ì‚° ë°©ì‹: {distance_metric}")
    logger.info(f"ìµœì†Œ ì…€ ìˆ˜: {min_cells_per_region}")

    total_start = time()

    # OSRM ê±°ë¦¬ ìºì‹œ ì´ˆê¸°í™”
    osrm_cache = None
    if use_osrm_cache and distance_metric in ["osrm", "osrm_then_haversine"]:
        safe_region_name = (
            region_name.replace("íŠ¹ë³„ì‹œ", "").replace(",", "").replace(" ", "_")
        )
        osrm_cache = OSRMDistanceCache(
            cache_dir=osrm_cache_dir, region_name=safe_region_name
        )
        osrm_cache.load_cache()
        logger.info(f"OSRM ê±°ë¦¬ ìºì‹œ í™œì„±í™”: {safe_region_name}")
    else:
        logger.info("OSRM ê±°ë¦¬ ìºì‹œ ë¹„í™œì„±í™”")

    # ê¸°ë³¸ íŒŒì¼ëª… ì„¤ì •
    if filename is None:
        timestamp = datetime.now().strftime("%y%m%d%H%M")
        # ì§€ì—­ëª…ì—ì„œ íŠ¹ìˆ˜ë¬¸ì ì œê±°í•˜ì—¬ íŒŒì¼ëª…ì— ì‚¬ìš©
        safe_region_name = (
            region_name.replace("íŠ¹ë³„ì‹œ", "").replace(",", "").replace(" ", "_")
        )
        filename = f"{safe_region_name}_walking_regions_{timestamp}"

    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    if out_dir:
        output_path = Path(out_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        logger.info(f"ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_path.absolute()}")

    # 1. ì§€ì—­ ê²½ê³„ íšë“
    region_boundary = get_region_boundary(region_name)

    # 2. ìŒì‹ì  ë°ì´í„° ë¡œë“œ (ì„ íƒì‚¬í•­)
    restaurants_df = None
    if use_restaurant_data:
        restaurants_df = load_restaurant_data_with_dataloader(data_config, region_name)
        if not restaurants_df.empty:
            logger.info("ğŸ½ï¸ ìŒì‹ì  ë°ì´í„°ê°€ ê¶Œì—­ ìƒì„±ì— ë°˜ì˜ë©ë‹ˆë‹¤!")
        else:
            logger.warning("ìŒì‹ì  ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ì–´ ê¸°ë³¸ ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
    else:
        logger.info("ìŒì‹ì  ë°ì´í„°ê°€ ì œê³µë˜ì§€ ì•Šì•„ ê¸°ë³¸ ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")

    # 3. ì§€ì—­ ì „ì²´ë¥¼ H3ë¡œ ì»¤ë²„
    region_cell_ids = flood_fill_region_cells(region_boundary, resolution, region_name)

    # 4. H3 ì…€ ì •ë³´ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜ (ìŒì‹ì  ì •ë³´ í¬í•¨)
    cells_df = region_cells_to_dataframe(region_cell_ids, restaurants_df, resolution)

    # 4. ë„ë³´ ê±°ë¦¬ ê¸°ë°˜ ê·¸ë˜í”„ êµ¬ì„±
    logger.info("ë„ë³´ ê±°ë¦¬ ê¸°ë°˜ ê¶Œì—­ ê·¸ë˜í”„ êµ¬ì„± ì¤‘...")
    G, distance_cache = build_cell_graph(
        cells_df,
        distance_threshold_m=walking_threshold_m,
        distance_metric=distance_metric,
        osrm_base_url=osrm_base_url,
        osrm_profile=osrm_profile,
        osrm_timeout=osrm_timeout,
        kring=kring,
        osrm_cache=osrm_cache,
    )

    # 5. ì—°ê²° ìš”ì†Œ ê¸°ë°˜ ê¶Œì—­ ë¼ë²¨ë§ (í¬ê¸° ì œí•œ í¬í•¨)
    logger.info("ì—°ê²° ìš”ì†Œ ë¶„ì„ì„ í†µí•œ ê¶Œì—­ ìƒì„± ì¤‘...")
    region_labels = label_regions_from_components(
        G,
        min_cells_per_region=min_cells_per_region,
        max_region_distance_m=max_region_distance_m,
        max_cells_per_region=max_cells_per_region,
    )

    # 5.5. ê·¸ë˜í”„ ì¤‘ê°„ ì €ì¥ (ìˆ˜ì • ì „ ìƒíƒœ)
    if out_dir:
        logger.info("ê·¸ë˜í”„ ë¶„ì„ ê²°ê³¼ ì¤‘ê°„ ì €ì¥ ì¤‘...")
        saved_files = save_graph_analysis(
            G=G,
            cells_df=cells_df,
            region_labels=region_labels,
            distance_cache=distance_cache,
            out_dir=str(output_path),
            filename_prefix=f"{filename}_before_reassign",
        )
        logger.info(f"ì¤‘ê°„ ì €ì¥ ì™„ë£Œ: {len(saved_files)}ê°œ íŒŒì¼")

    # 6. ê³ ì•„ ì…€ ì¬í• ë‹¹ (ìƒˆë¡œìš´ ê¸°ëŠ¥)
    if enable_orphan_reassign:
        logger.info("ê³ ì•„ ì…€ ì¬í• ë‹¹ ì²˜ë¦¬ ì¤‘...")
        original_orphan_count = len(
            [rid for rid in region_labels.values() if rid == -1]
        )
        if original_orphan_count > 0:
            reassign_distance = max_reassign_distance_m or (walking_threshold_m * 1.5)
            region_labels = reassign_orphan_cells(
                G=G,
                cells_df=cells_df,
                region_labels=region_labels,
                max_reassign_distance_m=reassign_distance,
                osrm_cache=osrm_cache,
            )
            final_orphan_count = len(
                [rid for rid in region_labels.values() if rid == -1]
            )
            logger.info(
                f"ê³ ì•„ ì…€ ê°ì†Œ: {original_orphan_count} â†’ {final_orphan_count}ê°œ"
            )
        else:
            logger.info("ì¬í• ë‹¹í•  ê³ ì•„ ì…€ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        logger.info("ê³ ì•„ ì…€ ì¬í• ë‹¹ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")

    # 7. ìµœì¢… ê²°ê³¼ DataFrame ìƒì„±
    logger.info("ìµœì¢… ê²°ê³¼ ìƒì„± ì¤‘...")
    result_df = cells_df.copy()
    result_df["region_id"] = result_df["cell_id"].map(region_labels)

    # 8. íŒŒì¼ ì €ì¥ ë° ìµœì¢… ê·¸ë˜í”„ ì €ì¥
    if out_dir:
        logger.info("ê²°ê³¼ íŒŒì¼ ì €ì¥ ì¤‘...")
        csv_path = output_path / f"{filename}.csv"
        geojson_path = output_path / f"{filename}.geojson"
        result_df.to_csv(csv_path, index=False, encoding="utf-8")
        cells_to_geojson(result_df, str(geojson_path))
        logger.info(f"ì €ì¥ ì™„ë£Œ: {csv_path.name}, {geojson_path.name}")

        # ìµœì¢… ê·¸ë˜í”„ ìƒíƒœ ì €ì¥ (ì¬í• ë‹¹ í›„)
        final_saved_files = save_graph_analysis(
            G=G,
            cells_df=cells_df,
            region_labels=region_labels,
            distance_cache=distance_cache,
            out_dir=str(output_path),
            filename_prefix=f"{filename}_final",
        )
        logger.info(f"ìµœì¢… ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ: {len(final_saved_files)}ê°œ íŒŒì¼")

    # ìºì‹œ ì €ì¥
    if osrm_cache:
        osrm_cache.save_cache()

    # 8. ê²°ê³¼ í†µê³„
    n_cells = len(result_df)
    n_regions = result_df["region_id"].nunique()
    if -1 in result_df["region_id"].unique():
        n_regions -= 1  # ë…¸ì´ì¦ˆ ì œì™¸
    noise_cells = len(result_df[result_df["region_id"] == -1])
    valid_regions = result_df[result_df["region_id"] >= 0]
    avg_cells_per_region = len(valid_regions) / n_regions if n_regions > 0 else 0

    # ìŒì‹ì  í†µê³„ (ìˆëŠ” ê²½ìš°)
    restaurant_stats = {}
    if "restaurant_count" in result_df.columns:
        restaurant_stats = {
            "total_restaurants": result_df["restaurant_count"].sum(),
            "cells_with_restaurants": (result_df["restaurant_count"] > 0).sum(),
            "avg_restaurants_per_cell": result_df["restaurant_count"].mean(),
            "avg_rating": result_df[result_df["avg_rating"] > 0]["avg_rating"].mean()
            if (result_df["avg_rating"] > 0).any()
            else 0,
        }

    total_elapsed = time() - total_start
    logger.info("=" * 60)
    logger.info(f"{region_name} ë„ë³´ ê¶Œì—­ ìƒì„± ì™„ë£Œ!")
    logger.info(f"ì´ H3 ì…€: {n_cells}ê°œ")
    logger.info(f"ìƒì„±ëœ ê¶Œì—­: {n_regions}ê°œ")
    logger.info(f"ë…¸ì´ì¦ˆ ì…€: {noise_cells}ê°œ")
    logger.info(f"ê¶Œì—­ë‹¹ í‰ê·  ì…€ ìˆ˜: {avg_cells_per_region:.1f}ê°œ")

    # ìŒì‹ì  í†µê³„ ì¶œë ¥
    if restaurant_stats:
        logger.info(f"ğŸ½ï¸ ì´ ìŒì‹ì : {restaurant_stats['total_restaurants']:,}ê°œ")
        logger.info(
            f"ğŸ½ï¸ ìŒì‹ì ì´ ìˆëŠ” ì…€: {restaurant_stats['cells_with_restaurants']:,}ê°œ"
        )
        logger.info(
            f"ğŸ½ï¸ ì…€ë‹¹ í‰ê·  ìŒì‹ì : {restaurant_stats['avg_restaurants_per_cell']:.1f}ê°œ"
        )
        if restaurant_stats["avg_rating"] > 0:
            logger.info(f"â­ í‰ê·  í‰ì : {restaurant_stats['avg_rating']:.2f}")

    logger.info(f"ì´ ì†Œìš”ì‹œê°„: {total_elapsed:.2f}s")
    logger.info("=" * 60)

    return result_df
