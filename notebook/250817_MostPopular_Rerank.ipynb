{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c554fe5c",
   "metadata": {},
   "source": [
    "\n",
    "# Most Popular Re-ranking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8b966e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROOT_PATH: C:\\Users\\LEEYS\\Desktop\\yamyam-lab\\src\n",
      "CONFIG_PATH: C:\\Users\\LEEYS\\Desktop\\yamyam-lab\\config/models/mf/als.yaml\n",
      "PREPROCESS_CONFIG_PATH: C:\\Users\\LEEYS\\Desktop\\yamyam-lab\\config/preprocess/preprocess.yaml\n",
      "RESULT_ROOT: C:\\Users\\LEEYS\\Desktop\\yamyam-lab\\result/untest/most_popular/20250817232110\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === 경로/설정 ===\n",
    "from pathlib import Path\n",
    "import os, sys, traceback\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "dt = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "test = \"untest\"\n",
    "model = \"most_popular\"\n",
    "config_model = \"als\"  \n",
    "\n",
    "\n",
    "PARENTS_PATH = Path(os.getcwd()).resolve().parent\n",
    "CONFIG_PATH = os.path.join(PARENTS_PATH, f\"config/models/mf/{config_model}.yaml\")\n",
    "PREPROCESS_CONFIG_PATH = os.path.join(PARENTS_PATH, f\"config/preprocess/preprocess.yaml\")\n",
    "RESULT_PATH = os.path.join(PARENTS_PATH, f\"result/{test}/{model}/{dt}\")\n",
    "\n",
    "ROOT_PATH = os.path.join(PARENTS_PATH,'src')\n",
    "\n",
    "\n",
    "if str(ROOT_PATH) not in sys.path:\n",
    "    sys.path.append(str(ROOT_PATH))\n",
    "\n",
    "print(f\"ROOT_PATH: {ROOT_PATH}\")\n",
    "print(f\"CONFIG_PATH: {CONFIG_PATH}\")\n",
    "print(f\"PREPROCESS_CONFIG_PATH: {PREPROCESS_CONFIG_PATH}\")\n",
    "print(f\"RESULT_ROOT: {RESULT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b2fc15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project imports succeeded.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "PROJECT_MODE = True\n",
    "try:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    from data.dataset import DataConfig, DatasetLoader\n",
    "    from evaluation.metric_calculator import MostPopularMetricCalculator\n",
    "    from tools.config import load_yaml\n",
    "    from tools.logger import common_logging, setup_logger\n",
    "    from tools.parse_args import save_command_to_file\n",
    "\n",
    "    print(\"Project imports succeeded.\")\n",
    "except Exception as e:\n",
    "    PROJECT_MODE = False\n",
    "    print(\"Project imports failed.\")\n",
    "    print(\"Reason:\", repr(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60343826",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Optional, Tuple, Iterable\n",
    "\n",
    "def _haversine_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    dlat = np.radians(lat2 - lat1); dlon = np.radians(lon2 - lon1)\n",
    "    a = np.sin(dlat/2)**2 + np.cos(np.radians(lat1))*np.cos(np.radians(lat2))*np.sin(dlon/2)**2\n",
    "    return 2 * R * np.arcsin(np.sqrt(a))\n",
    "\n",
    "def _geo_similarity_km(latlon_i, latlon_j, tau_km: float):\n",
    "    li = np.asarray(latlon_i, dtype=float)\n",
    "    lj = np.asarray(latlon_j, dtype=float)\n",
    "    if li.shape != (2,) or lj.shape != (2,):\n",
    "        return 0.0\n",
    "    if np.any(np.isnan(li)) or np.any(np.isnan(lj)): \n",
    "        return 0.0\n",
    "\n",
    "    R = 6371.0\n",
    "    dlat = np.radians(lj[0] - li[0])\n",
    "    dlon = np.radians(lj[1] - li[1])\n",
    "    a = np.sin(dlat/2)**2 + np.cos(np.radians(li[0]))*np.cos(np.radians(lj[0]))*np.sin(dlon/2)**2\n",
    "    dist = 2 * R * np.arcsin(np.sqrt(a))\n",
    "    return float(np.exp(-dist / max(tau_km, 1e-6)))\n",
    "\n",
    "def extract_region_label(addr: str) -> str:\n",
    "    if not isinstance(addr, str) or not addr:\n",
    "        return \"unknown\"\n",
    "    parts = addr.split()\n",
    "    if len(parts) >= 2 and parts[1].endswith(\"구\"):\n",
    "        return \" \".join(parts[:2])\n",
    "    m = re.match(r\"^(\\S+)\\s+(\\S+구|\\S+군|\\S+시)\", addr)\n",
    "    return m.group(0) if m else parts[0] if parts else \"unknown\"\n",
    "\n",
    "def build_item_meta_for_rerank(diner_meta: pd.DataFrame) -> pd.DataFrame:\n",
    "    meta = diner_meta.copy()\n",
    "    if \"region\" not in meta.columns and \"diner_road_address\" in meta.columns:\n",
    "        meta[\"region\"] = meta[\"diner_road_address\"].map(extract_region_label)\n",
    "\n",
    "    if \"lat\" not in meta.columns: meta[\"lat\"] = np.nan\n",
    "    if \"lon\" not in meta.columns: meta[\"lon\"] = np.nan\n",
    "\n",
    "    meta[\"lat\"] = pd.to_numeric(meta[\"lat\"], errors=\"coerce\")\n",
    "    meta[\"lon\"] = pd.to_numeric(meta[\"lon\"], errors=\"coerce\")\n",
    "\n",
    "    return meta[[\"diner_idx\", \"category\", \"lat\", \"lon\", \"region\"]].drop_duplicates(\"diner_idx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "165e7dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _pairwise_sim_max(i_meta: pd.Series, selected_meta: List[pd.Series], w_cat: float, w_geo: float, geo_tau_km: float) -> float:\n",
    "    if not selected_meta:\n",
    "        return 0.0\n",
    "    cat_i = i_meta.get(\"category\", \"unknown\")\n",
    "    latlon_i = (i_meta.get(\"lat\", np.nan), i_meta.get(\"lon\", np.nan))\n",
    "    sims = []\n",
    "    for j_meta in selected_meta:\n",
    "        cat_sim = 1.0 if (cat_i == j_meta.get(\"category\", None)) else 0.0\n",
    "        geo_sim = _geo_similarity_km(latlon_i, (j_meta.get(\"lat\", np.nan), j_meta.get(\"lon\", np.nan)), geo_tau_km)\n",
    "        sims.append(w_cat * cat_sim + w_geo * geo_sim)\n",
    "    return float(np.max(sims))\n",
    "\n",
    "def _apply_coverage_constraints(\n",
    "    cand_ids: np.ndarray,\n",
    "    chosen_ids: List[int],\n",
    "    item_meta_idx: pd.DataFrame,\n",
    "    coverage_min: dict,\n",
    "    coverage_max: dict,\n",
    "    region_of: dict,\n",
    "):\n",
    "    if not coverage_min and not coverage_max:\n",
    "        return cand_ids\n",
    "\n",
    "    def _labels_of(item_id: int):\n",
    "        if item_id not in item_meta_idx.index:\n",
    "            labels = [\"category:unknown\"]\n",
    "            if region_of: labels.append(f\"region:{region_of.get(int(item_id), 'unknown')}\")\n",
    "            return labels\n",
    "        row = item_meta_idx.loc[item_id]\n",
    "        labels = [f\"category:{row['category']}\"]\n",
    "        if region_of: labels.append(f\"region:{region_of.get(int(item_id), 'unknown')}\")\n",
    "        return labels\n",
    "\n",
    "    counts = {}\n",
    "    for cid in chosen_ids:\n",
    "        for lab in _labels_of(int(cid)):\n",
    "            counts[lab] = counts.get(lab, 0) + 1\n",
    "\n",
    "    keep = []\n",
    "    for cid in cand_ids:\n",
    "        labs = _labels_of(int(cid))\n",
    "        ok = True\n",
    "        if coverage_max:\n",
    "            for lab, mx in coverage_max.items():\n",
    "                if lab in labs and counts.get(lab, 0) >= mx:\n",
    "                    ok = False; break\n",
    "        if ok: keep.append(cid)\n",
    "    return np.array(keep, dtype=cand_ids.dtype)\n",
    "\n",
    "def rerank_most_popular_with_diversity(\n",
    "    item_ids: np.ndarray,\n",
    "    base_scores: np.ndarray,\n",
    "    item_meta: pd.DataFrame,\n",
    "    k: int,\n",
    "    lambda_div: float = 0.35,\n",
    "    w_cat: float = 0.6,\n",
    "    w_geo: float = 0.4,\n",
    "    geo_tau_km: float = 2.0,\n",
    "    coverage_min: Optional[Dict[str, int]] = None,\n",
    "    coverage_max: Optional[Dict[str, int]] = None,\n",
    "    region_of: Optional[Dict[int, str]] = None,\n",
    "    popularity_weight: float = 0.5,\n",
    "    popularity_scores: Optional[np.ndarray] = None,\n",
    "):\n",
    "    assert len(item_ids) == len(base_scores), \"item_ids/base_scores 길이 불일치\"\n",
    "    L = len(item_ids); k = min(k, L)\n",
    "\n",
    "    rel = base_scores.astype(float).copy()\n",
    "    if popularity_scores is not None:\n",
    "        def _norm(x):\n",
    "            x = x - np.min(x); denom = np.max(x)\n",
    "            return x / denom if denom > 0 else np.zeros_like(x)\n",
    "        rel = (1 - popularity_weight) * _norm(rel) + popularity_weight * _norm(popularity_scores.astype(float))\n",
    "\n",
    "    item_meta_idx = item_meta.set_index(\"diner_idx\", drop=False)\n",
    "\n",
    "    has_meta = np.array([cid in item_meta_idx.index for cid in item_ids])\n",
    "    if not has_meta.all():\n",
    "        item_ids = item_ids[has_meta]\n",
    "        rel = rel[has_meta]\n",
    "\n",
    "    chosen_ids: List[int] = []; chosen_scores: List[float] = []; selected_meta: List[pd.Series] = []\n",
    "    remaining = item_ids.copy()\n",
    "\n",
    "    while len(chosen_ids) < k and len(remaining) > 0:\n",
    "        remaining = _apply_coverage_constraints(\n",
    "            cand_ids=remaining,\n",
    "            chosen_ids=chosen_ids,\n",
    "            item_meta_idx=item_meta_idx,\n",
    "            coverage_min=coverage_min or {},\n",
    "            coverage_max=coverage_max or {},\n",
    "            region_of=region_of or {},\n",
    "        )\n",
    "        if len(remaining) == 0: break\n",
    "\n",
    "        mmr_scores = []\n",
    "        for cid in remaining:\n",
    "            base_rel = rel[np.where(item_ids == cid)[0][0]]\n",
    "            row = item_meta_idx.loc[int(cid)]\n",
    "            div_penalty = _pairwise_sim_max(row, selected_meta, w_cat, w_geo, geo_tau_km)\n",
    "            score = (1 - lambda_div) * base_rel - lambda_div * div_penalty\n",
    "\n",
    "            if coverage_min:\n",
    "                counts = {}\n",
    "                for cc in chosen_ids:\n",
    "                    crow = item_meta_idx.loc[int(cc)]\n",
    "                    clabs = [f\"category:{crow['category']}\"]\n",
    "                    if region_of: clabs.append(f\"region:{region_of.get(int(cc), 'unknown')}\")\n",
    "                    for lab in clabs: counts[lab] = counts.get(lab, 0) + 1\n",
    "                labs = [f\"category:{row['category']}\"]\n",
    "                if region_of: labs.append(f\"region:{region_of.get(int(cid), 'unknown')}\")\n",
    "                for lab, mn in (coverage_min or {}).items():\n",
    "                    if lab in labs and counts.get(lab, 0) < mn:\n",
    "                        score += (mn - counts.get(lab, 0)) * 0.05\n",
    "\n",
    "            mmr_scores.append(score)\n",
    "\n",
    "        mmr_scores = np.array(mmr_scores)\n",
    "        best_idx = int(np.argmax(mmr_scores))\n",
    "        best_id  = int(remaining[best_idx])\n",
    "\n",
    "        chosen_ids.append(best_id)\n",
    "        chosen_scores.append(float(mmr_scores[best_idx]))\n",
    "        selected_meta.append(item_meta_idx.loc[best_id])\n",
    "\n",
    "        remaining = remaining[remaining != best_id]\n",
    "\n",
    "    return np.array(chosen_ids, dtype=int), np.array(chosen_scores, dtype=float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bb9383",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Iterable, Tuple\n",
    "\n",
    "def rerank_region_periphery(\n",
    "    item_ids: np.ndarray,\n",
    "    base_scores: np.ndarray,\n",
    "    item_meta_std: pd.DataFrame,\n",
    "    k: int,\n",
    "    region_label: str = \"서울 강남구\",\n",
    "    hotspot_coords: Optional[Iterable[Tuple[float, float]]] = None,\n",
    "    n_auto_hotspots: int = 3,\n",
    "    periphery_strength: float = 0.25,\n",
    "    periphery_cap: float = 0.3,\n",
    "    lambda_div: float = 0.3,\n",
    "    w_cat: float = 0.6,\n",
    "    w_geo: float = 0.3,\n",
    "    geo_tau_km: float = 2.0,\n",
    "    coverage_min: Optional[Dict[str, int]] = None,\n",
    "    coverage_max: Optional[Dict[str, int]] = None,\n",
    "    region_of: Optional[Dict[int, str]] = None,\n",
    "):\n",
    "    meta = item_meta_std.copy()\n",
    "    gangnam_ids = set(meta.loc[meta[\"region\"] == region_label, \"diner_idx\"].tolist())\n",
    "    mask = np.array([iid in gangnam_ids for iid in item_ids])\n",
    "    item_ids_g = item_ids[mask]\n",
    "    base_scores_g = base_scores[mask]\n",
    "\n",
    "    if len(item_ids_g) == 0:\n",
    "        return item_ids[:k], base_scores[:k]\n",
    "\n",
    "    meta_idx = meta.set_index(\"diner_idx\")\n",
    "    latlon = meta_idx.loc[item_ids_g, [\"lat\", \"lon\"]].astype(float).values\n",
    "    valid = ~np.isnan(latlon).any(axis=1)\n",
    "    latlon_valid = latlon[valid]\n",
    "\n",
    "    if hotspot_coords is None:\n",
    "        from sklearn.cluster import KMeans\n",
    "        n_clusters = min(n_auto_hotspots, max(1, latlon_valid.shape[0]))\n",
    "        km = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
    "        km.fit(latlon_valid)\n",
    "        centers = km.cluster_centers_\n",
    "    else:\n",
    "        centers = np.array(list(hotspot_coords), dtype=float)\n",
    "\n",
    "    periphery_bonus = np.zeros_like(base_scores_g, dtype=float)\n",
    "    if len(centers) > 0 and latlon_valid.shape[0] > 0:\n",
    "        dists_min = []\n",
    "        for (lat, lon) in latlon_valid:\n",
    "            dmin = np.min([_haversine_km(lat, lon, c[0], c[1]) for c in centers])\n",
    "            dists_min.append(dmin)\n",
    "        dists_min = np.array(dists_min)\n",
    "\n",
    "        if dists_min.ptp() > 0:\n",
    "            d_norm = (dists_min - dists_min.min()) / (dists_min.max() - dists_min.min())\n",
    "        else:\n",
    "            d_norm = np.zeros_like(dists_min)\n",
    "\n",
    "        bonus_valid = np.clip(periphery_strength * d_norm, 0.0, periphery_cap)\n",
    "        periphery_bonus[valid] = bonus_valid\n",
    "\n",
    "    base_scores_boosted = base_scores_g + periphery_bonus\n",
    "\n",
    "    final_ids, final_scores = rerank_most_popular_with_diversity(\n",
    "        item_ids=item_ids_g,\n",
    "        base_scores=base_scores_boosted,\n",
    "        item_meta=meta,\n",
    "        k=k,\n",
    "        lambda_div=lambda_div,\n",
    "        w_cat=w_cat,\n",
    "        w_geo=w_geo if np.isfinite(latlon).any() else 0.0,\n",
    "        geo_tau_km=geo_tau_km,\n",
    "        coverage_min=coverage_min,\n",
    "        coverage_max=coverage_max,\n",
    "        region_of=region_of,\n",
    "        popularity_weight=0.0,\n",
    "        popularity_scores=None,\n",
    "    )\n",
    "    return final_ids, final_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a45ff5",
   "metadata": {},
   "source": [
    "\n",
    "## Project Mode (Main)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9912ed49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 23:23:18,292 - yamyam - INFO - model: most_popular\n",
      "2025-08-17 23:23:18,292 - yamyam - INFO - training results will be saved in C:\\Users\\LEEYS\\Desktop\\yamyam-lab\\result\\untest\\most_popular\\20250817232110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 data가 존재합니다. 파일 경로를 반환합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 23:31:00,933 - preprocess.filter - INFO - Token time for tokenizing: 421.44\n",
      "2025-08-17 23:31:02,467 - preprocess.filter - INFO - Detected 10 diner_ids with abusive reviews: [20557155.0, 561814157.0, 717255023.0, 1210281986.0, 1210386151.0, 1275807781.0, 1390211388.0, 1420824177.0, 1567102742.0, 1983344097.0]\n",
      "2025-08-17 23:31:02,663 - preprocess.filter - INFO - Excluded 3204 abusive reviews\n",
      "2025-08-17 23:31:05,489 - yamyam - INFO - train dataset period: 2024-09-01 <= dt < 2024-12-01\n",
      "2025-08-17 23:31:05,489 - yamyam - INFO - val dataset period: 2024-12-01 <= dt < 2025-01-01\n",
      "2025-08-17 23:31:05,489 - yamyam - INFO - test dataset period: 2025-01-01 <= dt < 2025-02-01\n",
      "2025-08-17 23:31:05,501 - yamyam - INFO - ######## Number of reviews statistics ########\n",
      "2025-08-17 23:31:05,501 - yamyam - INFO - Number of reviews in train: 666811\n",
      "2025-08-17 23:31:05,501 - yamyam - INFO - Number of reviews in val: 666811\n",
      "2025-08-17 23:31:05,501 - yamyam - INFO - Number of reviews in test: 666811\n",
      "2025-08-17 23:31:05,501 - yamyam - INFO - ######## Train data statistics ########\n",
      "2025-08-17 23:31:05,501 - yamyam - INFO - Number of users in train: 94799\n",
      "2025-08-17 23:31:05,501 - yamyam - INFO - Number of diners in train: 71281\n",
      "2025-08-17 23:31:05,501 - yamyam - INFO - Number of feedbacks in train: 666811\n",
      "2025-08-17 23:31:05,501 - yamyam - INFO - Train data density: 0.0099%\n",
      "2025-08-17 23:31:05,501 - yamyam - INFO - ######## Validation data statistics ########\n",
      "2025-08-17 23:31:05,501 - yamyam - INFO - Number of users in val: 35225\n",
      "2025-08-17 23:31:05,511 - yamyam - INFO - Number of diners in val: 33142\n",
      "2025-08-17 23:31:05,512 - yamyam - INFO - Number of feedbacks in val: 666811\n",
      "2025-08-17 23:31:05,513 - yamyam - INFO - Validation data density: 0.0571%\n",
      "2025-08-17 23:31:05,514 - yamyam - INFO - ######## Test data statistics ########\n",
      "2025-08-17 23:31:05,514 - yamyam - INFO - Number of users in test: 36037\n",
      "2025-08-17 23:31:05,514 - yamyam - INFO - Number of diners in test: 34788\n",
      "2025-08-17 23:31:05,514 - yamyam - INFO - Number of feedbacks in test: 666811\n",
      "2025-08-17 23:31:05,514 - yamyam - INFO - Test data density: 0.0532%\n",
      "2025-08-17 23:31:05,514 - yamyam - INFO - ######## Warm / Cold users analysis in validation and test dataset ########\n",
      "2025-08-17 23:31:05,535 - yamyam - INFO - Number of users within train, but not in val: 83994\n",
      "2025-08-17 23:31:05,555 - yamyam - INFO - Number of users within train, but not in test: 84347\n",
      "2025-08-17 23:31:05,556 - yamyam - INFO - Number of warm start users in val: 10805\n",
      "2025-08-17 23:31:05,556 - yamyam - INFO - Number of cold start users in val: 24420\n",
      "2025-08-17 23:31:05,557 - yamyam - INFO - Number of warm start users in test: 10452\n",
      "2025-08-17 23:31:05,558 - yamyam - INFO - Number of cold start users in test: 25585\n",
      "  File \"c:\\Users\\LEEYS\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\src-fg4b1aLu-py3.12\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "               ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\LEEYS\\anaconda3\\Lib\\subprocess.py\", line 548, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\LEEYS\\anaconda3\\Lib\\subprocess.py\", line 1026, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"C:\\Users\\LEEYS\\anaconda3\\Lib\\subprocess.py\", line 1538, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "2025-08-17 23:31:16,301 - yamyam - INFO - ################################ Validation data metric report ################################\n",
      "2025-08-17 23:31:16,301 - yamyam - INFO - [ warm users metric report calculated from val data ]\n",
      "2025-08-17 23:31:16,301 - yamyam - INFO - top k results for direct prediction @3, @7, @10, @20 in order\n",
      "2025-08-17 23:31:16,301 - yamyam - INFO - map result: 0.00018|0.00021|0.00023|0.00027\n",
      "2025-08-17 23:31:16,301 - yamyam - INFO - ndcg result: 0.00024|0.0003|0.00037|0.00052\n",
      "2025-08-17 23:31:16,301 - yamyam - INFO - top k results for candidate generation @100, @300, @500, @1000, @2000\n",
      "2025-08-17 23:31:16,306 - yamyam - INFO - recall: 0.00691|0.01049|0.01049|0.01049|0.01049\n",
      "2025-08-17 23:31:16,306 - yamyam - INFO - [ cold users metric report calculated from val data ]\n",
      "2025-08-17 23:31:16,306 - yamyam - INFO - top k results for direct prediction @3, @7, @10, @20 in order\n",
      "2025-08-17 23:31:16,306 - yamyam - INFO - map result: 0.00035|0.00051|0.00052|0.00068\n",
      "2025-08-17 23:31:16,306 - yamyam - INFO - ndcg result: 0.0004|0.0007|0.00074|0.00125\n",
      "2025-08-17 23:31:16,309 - yamyam - INFO - top k results for candidate generation @100, @300, @500, @1000, @2000\n",
      "2025-08-17 23:31:16,310 - yamyam - INFO - recall: 0.00827|0.0108|0.0108|0.0108|0.0108\n",
      "2025-08-17 23:31:16,311 - yamyam - INFO - [ all users metric report calculated from val data ]\n",
      "2025-08-17 23:31:16,312 - yamyam - INFO - top k results for direct prediction @3, @7, @10, @20 in order\n",
      "2025-08-17 23:31:16,312 - yamyam - INFO - map result: 0.0003|0.00042|0.00043|0.00055\n",
      "2025-08-17 23:31:16,312 - yamyam - INFO - ndcg result: 0.00035|0.00058|0.00063|0.00102\n",
      "2025-08-17 23:31:16,312 - yamyam - INFO - top k results for candidate generation @100, @300, @500, @1000, @2000\n",
      "2025-08-17 23:31:16,312 - yamyam - INFO - recall: 0.00786|0.0107|0.0107|0.0107|0.0107\n",
      "2025-08-17 23:31:17,444 - yamyam - INFO - ################################ Test data metric report ################################\n",
      "2025-08-17 23:31:17,445 - yamyam - INFO - [ warm users metric report calculated from test data ]\n",
      "2025-08-17 23:31:17,445 - yamyam - INFO - top k results for direct prediction @3, @7, @10, @20 in order\n",
      "2025-08-17 23:31:17,446 - yamyam - INFO - map result: 0.00016|0.00019|0.00022|0.00026\n",
      "2025-08-17 23:31:17,447 - yamyam - INFO - ndcg result: 0.00019|0.00024|0.00034|0.00056\n",
      "2025-08-17 23:31:17,448 - yamyam - INFO - top k results for candidate generation @100, @300, @500, @1000, @2000\n",
      "2025-08-17 23:31:17,448 - yamyam - INFO - recall: 0.00735|0.00961|0.00961|0.00961|0.00961\n",
      "2025-08-17 23:31:17,449 - yamyam - INFO - [ cold users metric report calculated from test data ]\n",
      "2025-08-17 23:31:17,450 - yamyam - INFO - top k results for direct prediction @3, @7, @10, @20 in order\n",
      "2025-08-17 23:31:17,451 - yamyam - INFO - map result: 0.00022|0.00041|0.00042|0.00065\n",
      "2025-08-17 23:31:17,451 - yamyam - INFO - ndcg result: 0.00026|0.00062|0.00063|0.00138\n",
      "2025-08-17 23:31:17,452 - yamyam - INFO - top k results for candidate generation @100, @300, @500, @1000, @2000\n",
      "2025-08-17 23:31:17,453 - yamyam - INFO - recall: 0.0097|0.01506|0.01506|0.01506|0.01506\n",
      "2025-08-17 23:31:17,454 - yamyam - INFO - [ all users metric report calculated from test data ]\n",
      "2025-08-17 23:31:17,454 - yamyam - INFO - top k results for direct prediction @3, @7, @10, @20 in order\n",
      "2025-08-17 23:31:17,454 - yamyam - INFO - map result: 0.0002|0.00035|0.00036|0.00054\n",
      "2025-08-17 23:31:17,455 - yamyam - INFO - ndcg result: 0.00024|0.00051|0.00055|0.00114\n",
      "2025-08-17 23:31:17,456 - yamyam - INFO - top k results for candidate generation @100, @300, @500, @1000, @2000\n",
      "2025-08-17 23:31:17,457 - yamyam - INFO - recall: 0.00902|0.01348|0.01348|0.01348|0.01348\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project pipeline finished. See logs for details: C:\\Users\\LEEYS\\Desktop\\yamyam-lab\\result\\untest\\most_popular\\20250817232110\\log.log\n"
     ]
    }
   ],
   "source": [
    "if PROJECT_MODE:\n",
    "    ROOT_PATH = Path(ROOT_PATH)\n",
    "    RESULT_PATH = Path(RESULT_PATH)\n",
    "\n",
    "    RESULT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    config = load_yaml(CONFIG_PATH)\n",
    "    preprocess_config = load_yaml(str(PREPROCESS_CONFIG_PATH))\n",
    "\n",
    "    # 로그 세팅 및 실행 커맨드 기록\n",
    "    file_name = config.post_training.file_name\n",
    "    logger = setup_logger(str(RESULT_PATH / file_name.log))\n",
    "    save_command_to_file(str(RESULT_PATH))\n",
    "\n",
    "    try:\n",
    "        logger.info(\"model: most_popular\")\n",
    "        logger.info(f\"training results will be saved in {RESULT_PATH}\")\n",
    "\n",
    "        # 데이터 로딩\n",
    "        fe = config.preprocess.feature_engineering\n",
    "        data_config=DataConfig(\n",
    "                X_columns=[\"diner_idx\", \"reviewer_id\"],\n",
    "                y_columns=[\"reviewer_review_score\"],\n",
    "                user_engineered_feature_names=fe.user_engineered_feature_names,\n",
    "                diner_engineered_feature_names=fe.diner_engineered_feature_names,\n",
    "                is_timeseries_by_time_point=config.preprocess.data.is_timeseries_by_time_point,\n",
    "                train_time_point=config.preprocess.data.train_time_point,\n",
    "                val_time_point=config.preprocess.data.val_time_point,\n",
    "                test_time_point=config.preprocess.data.test_time_point,\n",
    "                end_time_point=config.preprocess.data.end_time_point,\n",
    "                test=False,\n",
    "            )\n",
    "        data_config.additional_reviews_path = PARENTS_PATH / data_config.additional_reviews_path \n",
    "        data_loader = DatasetLoader(data_config = data_config)\n",
    "        data = data_loader.prepare_train_val_dataset(is_csr=True, filter_config=preprocess_config.filter)\n",
    "        common_logging(config=config, data=data, logger=logger)\n",
    "\n",
    "        # 평가 K 설정\n",
    "        top_k_values_for_pred = config.training.evaluation.top_k_values_for_pred\n",
    "        top_k_values_for_candidate = config.training.evaluation.top_k_values_for_candidate\n",
    "        top_k_values = top_k_values_for_pred + top_k_values_for_candidate\n",
    "\n",
    "        # 메타 구성\n",
    "        item_meta = build_item_meta_for_rerank(data[\"diner_meta\"])\n",
    "\n",
    "        N = max(2000, max(top_k_values))\n",
    "        candidates = np.array(data[\"most_popular_diner_ids\"][:N], dtype=int)\n",
    "        base_scores = 1.0 / (np.arange(len(candidates)) + 1)\n",
    "\n",
    "        valid = np.intersect1d(candidates, item_meta[\"diner_idx\"].values)\n",
    "        mask = np.isin(candidates, valid)\n",
    "        if len(valid) < len(candidates):\n",
    "            logger.warning(f\"dropped {len(candidates)-len(valid)} candidates not in item_meta\")\n",
    "        candidates = candidates[mask]; base_scores = base_scores[mask]\n",
    "\n",
    "        reranked_ids, _ = rerank_region_periphery(\n",
    "            item_ids=candidates,\n",
    "            base_scores=base_scores,\n",
    "            item_meta_std=item_meta,\n",
    "            k=max(top_k_values),\n",
    "            region_label=\"서울 강남구\",\n",
    "            hotspot_coords=None,\n",
    "            n_auto_hotspots=3,\n",
    "            periphery_strength=0.25,\n",
    "            periphery_cap=0.3,\n",
    "            lambda_div=0.3,\n",
    "            w_cat=0.6,\n",
    "            w_geo=0.3,\n",
    "            geo_tau_km=2.0,\n",
    "        )\n",
    "        reranked_most_popular = reranked_ids.tolist()\n",
    "\n",
    "        # 평가\n",
    "        metric_calculator = MostPopularMetricCalculator(\n",
    "            top_k_values=top_k_values,\n",
    "            filter_already_liked=True,\n",
    "            recommend_batch_size=config.training.evaluation.recommend_batch_size,\n",
    "            logger=logger,\n",
    "        )\n",
    "\n",
    "        metric_dict = metric_calculator.generate_recommendations_and_calculate_metric(\n",
    "            X_train=data[\"X_train_df\"],\n",
    "            X_val_warm_users=data[\"X_val_warm_users\"],\n",
    "            X_val_cold_users=data[\"X_val_cold_users\"],\n",
    "            most_popular_diner_ids=reranked_most_popular,\n",
    "            filter_already_liked=True,\n",
    "            most_popular_rec_to_warm_users=True,\n",
    "        )\n",
    "        for user_type, metric in metric_dict.items():\n",
    "            metric_calculator.calculate_mean_metric(metric)\n",
    "        logger.info(\"################################ Validation data metric report ################################\")\n",
    "        metric_calculator.report_metric_with_warm_cold_all_users(metric_dict=metric_dict, data_type=\"val\")\n",
    "\n",
    "        metric_dict = metric_calculator.generate_recommendations_and_calculate_metric(\n",
    "            X_train=data[\"X_train_df\"],\n",
    "            X_val_warm_users=data[\"X_test_warm_users\"],\n",
    "            X_val_cold_users=data[\"X_test_cold_users\"],\n",
    "            most_popular_diner_ids=reranked_most_popular,\n",
    "            filter_already_liked=True,\n",
    "            most_popular_rec_to_warm_users=True,\n",
    "        )\n",
    "        for user_type, metric in metric_dict.items():\n",
    "            metric_calculator.calculate_mean_metric(metric)\n",
    "        logger.info(\"################################ Test data metric report ################################\")\n",
    "        metric_calculator.report_metric_with_warm_cold_all_users(metric_dict=metric_dict, data_type=\"test\")\n",
    "\n",
    "        print(\"Project pipeline finished. See logs for details:\", RESULT_PATH / file_name.log)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error during project run:\", repr(e))\n",
    "        traceback.print_exc()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "src-fg4b1aLu-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
