# Running scripts for yamyam-lab

`scripts/` contains various scripts used when building recommender system in `yamyam-lab` repository.


| Python script                          | Description                                                        |
|----------------------------------------|--------------------------------------------------------------------|
| `scripts/create_google_drive_token.py` | Used when creating token.json in ci                                |
| `scripts/download_result.py`           | Used when downloading candidate generation or trained model result |
| `scripts/generate_candidate.py`        | Used when generating candidates from trained model                 |
| `scripts/build_regions.py` | Used when generating region cluster |
## How to download candidate generation or trained model result

Here, we run `scripts/download_result.py` python file to download results.

Note that you could directly download candidate results or trained model result in [google drive](https://drive.google.com/drive/u/0/folders/1kjoSmJ8bn3NIWbzJlPFXZkt6IFrcWIz4).

To download it using python code, please follow below steps.

1. Place credential file to authenticate google drive api to `credentials/` directory.
   - Refer to [this discussion](https://github.com/LearningnRunning/yamyam-lab/discussions/118#discussioncomment-12590729) about how to download credential json file from gcp.

2. Run following command depending on the embedding model and what you want to download, which is either of `candidates` or `models`.

    - If you want to download candidate generation results, place `--download_file_type` argument as `candidates`.
    - If you want to download trained model results with torch weight, logs, metric plot etc, place `--download_file_type` argument as `models`.
    - Currently, this script supports downloading latest result, which is denoted as `--latest` argument.

```bash
$ poetry run python3 scripts/download_result.py \
  --model_name "node2vec" \
  --download_file_type "candidates" \
  --latest \
  --credential_file_path_from_gcloud_console "PATH/TO/CREDENTIALS.json" \
  --reusable_token_path "PATH/TO/TOKEN.json"
```

Refer to description of each parameter.

| Parameter name                             | Description                                                                                |
|--------------------------------------------|--------------------------------------------------------------------------------------------|
| `model_name`                               | Name of embedding model (`node2vec` / `metapath2vec` / `graphsage` are allowed)            |
| `download_file_type`                       | File type you want to download (`candidates` / `models` are allowed)                       |
| `latest`                                   | Indicator whether downloading latest candidate generation results in selected model or not |
| `credential_file_path_from_gcloud_console` | Path to credential json file from gcp                                                      |
| `reusable_token_path`                      | Path to reusable token path                                                                |


After running script, check whether zip file is downloaded in `candidates/{model_name}` or `trained_models/{model_name}` successfully.

Latest version of each embedding model is given below. Note that identical version is applied both of candidate generation result and training result. (If zip file name is identical, they are generated from same training pipeline)

| Model name   | Latest version   |
|--------------|------------------|
| node2vec     | 202504070010.zip |
| metapath2vec | 202504011954.zip |
| graphsage    | 202504122051.zip |


## How to generate candidates from trained embedding model

Here, we run `scripts/generate_candidate.py` python file to generate candidates.

There are two required files when generating candidates from trained embedding model.

- weight.pt
- data_object.pkl

You could directly download those files in [google drive](https://drive.google.com/drive/u/0/folders/1zdqZldExdYZ2eH-Gfabnh8rHkWamPnVG) unzipping training results.

Or you could download them running `scripts/download_result.py` script. Please refer to above `How to download candidate generation or trained model result` section for more details.

You should specify path for trained pytorch weight and data object when running `scripts/generate_candidate.py`.

Depending on the embedding model you want, different arguments are required.

Refer to description of each parameter.

| Parameter name                     | Description                                                                     |
|------------------------------------|---------------------------------------------------------------------------------|
| `model`                            | Name of embedding model (`node2vec` / `metapath2vec` / `graphsage` / `lightgcn` are allowed) |
| `data_obj_path`                    | Path to data_object.pkl                                                         |
| `model_pt_path`                    | Path to weight.pt                                                               |
| `candidate_top_k`                  | Number of candidates to generate                                                |
| `reusable_token_path`              | Path to reusable token path                                                     |
| `upload_candidate_to_google_drive` | Indicator value whether to upload generated candidates to google drive or not   |

### node2vec

```bash
$ poetry run python3 scripts/generate_candidate.py \
  --model node2vec \
  --data_obj_path /PATH/TO/NODE2VEC/data_object.pkl \
  --model_pt_path /PATH/TO/NODE2VEC/weight.pt \
  --candidate_top_k 100 \
  --reusable_token_path PATH/TO/token.json
```

### metapath2vec
```bash
$ poetry run python3 scripts/generate_candidate.py \
  --model metapath2vec \
  --data_obj_path /PATH/TO/METAPATH2VEC/data_object.pkl \
  --model_pt_path /PATH/TO/METAPATH2VEC/weight.pt \
  --candidate_top_k 100 \
  --reusable_token_path PATH/TO/token.json
```


### graphsage

```bash
$ poetry run python3 scripts/generate_candidate.py \
  --model graphsage \
  --data_obj_path /PATH/TO/GRAPHSAGE/data_object.pkl \
  --model_pt_path /PATH/TO/GRAPHSAGE/weight.pkl \
  --candidate_top_k 100 \
  --reusable_token_path PATH/TO/token.json
```


## Region preprocessing (walking regions)

ì•„ë˜ ë‚´ìš©ì€ `src/preprocess/region/README.md`ì˜ í•µì‹¬ì„ ì¢…í•©í•˜ì—¬ `scripts/` ê´€ì ì—ì„œ ì •ë¦¬í•œ ê²ƒì…ë‹ˆë‹¤. ì„œìš¸ ë“± íŠ¹ì • ì§€ì—­ì„ H3 í•´ìƒë„ 10ìœ¼ë¡œ ì»¤ë²„í•œ ë’¤, ë„ë³´ ê±°ë¦¬ ê¸°ë°˜ìœ¼ë¡œ ì—°ê²°/í´ëŸ¬ìŠ¤í„°ë§í•˜ì—¬ ì¶”ì²œì— ì í•©í•œ ê¶Œì—­ì„ ìƒì„±í•©ë‹ˆë‹¤.

### ì£¼ìš” ê¸°ëŠ¥

- ğŸ™ï¸ OSMnxë¡œ íŠ¹ì • ì§€ì—­(ì˜ˆ: ì„œìš¸ì‹œ) í–‰ì •ê²½ê³„ ìë™ íšë“
- ğŸ“ H3 í•´ìƒë„ 10 ê¸°ë°˜ ì „ì²´ ì»¤ë²„ë¦¬ì§€ ìƒì„±
- ğŸš¶ OSRM ë„ë³´ ê±°ë¦¬ ê³„ì‚° (Haversine ë°±ì—…)
- ğŸ”— ê·¸ë˜í”„ ê¸°ë°˜ ì—°ê²° ìš”ì†Œ ë¶„ì„
- ğŸ“ CSV/GeoJSON ê²°ê³¼ ì¶œë ¥ ë° ì¤‘ê°„ ë¶„ì„ ì‚°ì¶œë¬¼ ì˜µì…˜ ì œê³µ
- ğŸ½ï¸ ìŒì‹ì  ë°ì´í„° í†µí•© (yamyam-lab DataLoader ì—°ë™)

### ë””ë ‰í„°ë¦¬ êµ¬ì¡°

```
src/preprocess/region/
â”œâ”€â”€ __init__.py          # ëª¨ë“ˆ ì´ˆê¸°í™” ë° ê³µê°œ API
â”œâ”€â”€ builder.py           # í•µì‹¬ ê¶Œì—­ ìƒì„± ë¡œì§
â””â”€â”€ README.md            # ìƒì„¸ ë¬¸ì„œ
```

### 1) CLI ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš© (ê¶Œì¥)

ê¶Œì—­ ìƒì„±ì€ `scripts/build_regions.py`ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.

```bash
# ì˜ë“±í¬êµ¬ í…ŒìŠ¤íŠ¸ (ë¹ ë¥¸ ì‹¤í–‰)
poetry run python scripts/build_regions.py --region "ì˜ë“±í¬êµ¬"

# ì„œìš¸ì‹œ ì „ì²´ ì‹¤í–‰
poetry run python scripts/build_regions.py --region "ì„œìš¸íŠ¹ë³„ì‹œ"

# ìŒì‹ì  ë°ì´í„° ì—†ì´ ì‹¤í–‰
poetry run python scripts/build_regions.py --region "ì˜ë“±í¬êµ¬" --no_restaurant_data

# ì„¸ë¶€ ì„¤ì • ì¡°ì • ì˜ˆì‹œ
poetry run python scripts/build_regions.py \
    --region "ì¤‘êµ¬" \
    --threshold_m 400 \
    --max_region_distance_m 1500 \
    --min_cells 20 \
    --max_cells 25 \
    --out_dir data/processed/regions/test
```

ì£¼ìš” ì¸ì ìš”ì•½:

- `--region`: í–‰ì •êµ¬ì—­ëª… (ì˜ˆ: "ì˜ë“±í¬êµ¬", "ì„œìš¸íŠ¹ë³„ì‹œ")
- `--threshold_m`: ë„ë³´ ì—°ê²° ì„ê³„ê°’(ë¯¸í„°)
- `--max_region_distance_m`: ê¶Œì—­ ë‚´ ìµœëŒ€ ì—°ê²° ê±°ë¦¬(ë¯¸í„°)
- `--min_cells`/`--max_cells`: ê¶Œì—­ í¬ê¸° í•˜í•œ/ìƒí•œ(H3 ì…€ ìˆ˜)
- `--no_restaurant_data`: ìŒì‹ì  ë°ì´í„° ë¯¸ì‚¬ìš© í”Œë˜ê·¸
- `--out_dir`: ê²°ê³¼ ì €ì¥ ë””ë ‰í„°ë¦¬

### 2) Python ì½”ë“œì—ì„œ ì§ì ‘ ì‚¬ìš©

ìŠ¤í¬ë¦½íŠ¸ ëŒ€ì‹  ë¼ì´ë¸ŒëŸ¬ë¦¬ì²˜ëŸ¼ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

```python
from data.config import DataConfig
from preprocess.region import build_walking_regions

# ë°ì´í„° ì„¤ì • ë¡œë“œ
data_config = DataConfig.from_yaml("config/data/default.yaml")

# ê¶Œì—­ ìƒì„± ì‹¤í–‰
result_df = build_walking_regions(
    data_config=data_config,
    region_name="ì˜ë“±í¬êµ¬",
    walking_threshold_m=500,
    max_region_distance_m=2000,
    out_dir="data/processed/regions",
    use_restaurant_data=True,
)

print(f"ìƒì„±ëœ ê¶Œì—­ ìˆ˜: {result_df['region_id'].nunique()}")
```

### ì¶œë ¥ íŒŒì¼

ê¶Œì—­ ìƒì„± ì™„ë£Œ í›„ ì•„ë˜ì™€ ê°™ì€ íŒŒì¼ë“¤ì´ ìƒì„±ë©ë‹ˆë‹¤.

- `{region}_walking_regions_{timestamp}.csv`: ê¶Œì—­ ì •ë³´ CSV
- `{region}_walking_regions_{timestamp}.geojson`: ì‹œê°í™”ìš© GeoJSON

ì„ íƒì  ì¤‘ê°„ ì‚°ì¶œë¬¼(ì˜µì…˜):

- `{prefix}_graph.pkl`: NetworkX ê·¸ë˜í”„ ê°ì²´
- `{prefix}_distance_cache.pkl`: ê±°ë¦¬ ê³„ì‚° ìºì‹œ
- `{prefix}_edges.csv`: ì—£ì§€ ë¦¬ìŠ¤íŠ¸
- `{prefix}_nodes.csv`: ë…¸ë“œ í†µê³„
- `{prefix}_region_stats.json`: ê¶Œì—­ í†µê³„

### ì„¤ì • íŒŒì¼

ê¸°ë³¸ ì„¤ì •ì€ `config/preprocess/region.yaml`ì—ì„œ ê´€ë¦¬í•©ë‹ˆë‹¤. ì£¼ìš” í•­ëª©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

- H3 í•´ìƒë„
- ë„ë³´ ê±°ë¦¬ ì„ê³„ê°’
- ìµœëŒ€ ê¶Œì—­ ê±°ë¦¬
- OSRM ì„¤ì •
- ìºì‹œ ì„¤ì •
- ì¶œë ¥ ì„¤ì •

### ì„±ëŠ¥ ìµœì í™” íŒ

- OSRM ìºì‹œ: ê±°ë¦¬ ê³„ì‚° ê²°ê³¼ë¥¼ `data/cache/osrm/osrm_distance_cache_{region}.pkl`ì— ìºì‹œí•˜ì—¬ ì¬ì‹¤í–‰ ì‹œ ì†ë„ë¥¼ í¬ê²Œ í–¥ìƒí•©ë‹ˆë‹¤.
- H3 K-ring ì „ëµ: k-ring ì´ì›ƒì— ëŒ€í•´ì„œë§Œ ê±°ë¦¬ ê³„ì‚°í•˜ì—¬ O(nÂ²) â†’ O(n) ìˆ˜ì¤€ìœ¼ë¡œ ì¤„ì…ë‹ˆë‹¤. ê¸°ë³¸ k=1 ê¶Œì¥.

### ì¶”ì²œ ì‹œìŠ¤í…œì—ì„œì˜ í™œìš© ì˜ˆ

```python
import pandas as pd
import h3

# ê¶Œì—­ ë°ì´í„° ë¡œë“œ
# TODO ëª¨ë“ˆë¡œ ì“¸ ìˆ˜ ìˆê²Œ ì¶”ê°€ ì˜ˆì •
regions_df = pd.read_csv("data/processed/regions/Seoul_walking_regions_latest.csv")

# íŠ¹ì • ì¢Œí‘œì˜ ê¶Œì—­ ì°¾ê¸°
def find_region(lat, lon, regions_df):
    cell_id = h3.latlng_to_cell(lat, lon, 10)
    region_info = regions_df[regions_df['cell_id'] == cell_id]
    return region_info['region_id'].iloc[0] if not region_info.empty else -1

# ê¶Œì—­ ê¸°ë°˜ ì¶”ì²œ í•„í„°ë§
user_region = find_region(user_lat, user_lon, regions_df)
nearby_restaurants = restaurants_df[restaurants_df['region_id'] == user_region]
```

ë³´ë‹¤ ìƒì„¸í•œ ë°°ê²½ê³¼ êµ¬í˜„ ì„¤ëª…ì€ `src/preprocess/region/README.md`ì™€ `src/preprocess/region/builder.py`ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.
